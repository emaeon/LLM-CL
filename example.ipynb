{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9267e0-1905-4e0a-90b0-63e7813128b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/main/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING: BNB_CUDA_VERSION=123 environment variable detected; loading libbitsandbytes_cuda121_nocublaslt123.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n",
      "Could not find the bitsandbytes CUDA binary at PosixPath('/home/work/anaconda3/envs/main/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121_nocublaslt123.so')\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig,HfArgumentParser, get_scheduler, set_seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'mode_ID':\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "          'seed': 1 ,\n",
    "          'max_seq_len' : 4096,\n",
    "          'epochs': 3,\n",
    "          'lr': 2e-4,\n",
    "          'batch': 4,\n",
    "          'lora_r':8,\n",
    "          'lora_alpha':32,\n",
    "          'target_module':[\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\",\"gate_proj\", \"v_proj\"],\n",
    "          'lora_dropout':0.05,\n",
    "          'lora_tasktype' :'CAUSAL_LM',\n",
    "          'lora_bias' : 'none',\n",
    "          'optimizer': 'paged_adamw_8bit',\n",
    "          'scheduler':'cosine'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,  # PEFT 설정을 가져오기 위한 함수\n",
    "    get_peft_model,  # PEFT 모델을 가져오기 위한 함수\n",
    "    get_peft_model_state_dict,  # PEFT 모델 상태 사전을 가져오기 위한 함수\n",
    "    set_peft_model_state_dict,  # PEFT 모델 상태 사전을 설정하기 위한 함수\n",
    "    LoraConfig,  # LoRA 모델 구성을 정의하는 클래스\n",
    "    PeftType,  # PEFT 모델의 타입을 정의\n",
    "    PrefixTuningConfig,  # PrefixTuning 모델 구성을 정의하는 클래스\n",
    "    PromptEncoderConfig,  # PromptEncoder 모델 구성을 정의하는 클래스\n",
    "    PeftModel,  # PEFT 모델을 정의하는 클래스\n",
    "    PeftConfig,  # PEFT 모델의 구성을 정의하는 클래스\n",
    ")\n",
    "\n",
    "# PEFT 모델의 타입 설정 (LoRA로 설정)\n",
    "peft_type = PeftType.LORA\n",
    "\n",
    "# LoRA 모델을 위한 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=config['lora_r'],  # LoRA 모델의 r 값\n",
    "    lora_alpha=config['lora_alpha'],  # LoRA 모델의 alpha 값\n",
    "    target_modules=config['target_module'],  # LoRA 모델의 타겟 모듈 리스트\n",
    "    lora_dropout=config['lora_dropout'],  # LoRA 모델의 드롭아웃 비율\n",
    "    bias=config['lora_bias'],  # LoRA 모델의 편향 설정\n",
    "    task_type=config['lora_tasktype']  # LoRA 모델의 태스크 유형\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed05f2fd-0929-4c62-8b7e-2c26eb3b0b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.11s/it]\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer를 사용하여 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['mode_ID'], trust_remote_code=True, eos_token='</s>')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig['mode_ID'],\n",
    "\tdevice_map=\"cuda\",\n",
    "\ttorch_dtype=torch.float16,\n",
    "\ttrust_remote_code=True, \n",
    "\tuse_cache=False,\n",
    "\t# quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable() # 모델에서 그래디언트 체크포인팅 활성화 (메모리 효율 향상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ba1119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3 크기 : 3821.1M개의 파라미터\n"
     ]
    }
   ],
   "source": [
    "print(f'Phi3 크기 : {model.num_parameters()/1000**2:.1f}M개의 파라미터')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b199022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.1165\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training # peft 라이브러리에서 k 비트 학습 준비 함수 임포트\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # CUDA 사용 가능 여부 확인\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)# k 비트 학습을 위해 모델 준비 - prepare_model_for_kbit_training 함수 사용\n",
    "model = get_peft_model(model, peft_config) # PEFT 적용 \n",
    "model = model.to(device) # 모델을 학습 장치 (GPU 등)로 이동\n",
    "model.print_trainable_parameters()# 훈련 가능한 파라미터 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c494c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save_pretrained('models/'+model_ckpt, push_to_hun=True, organiztion=org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23758421-c94e-463a-a25b-048854782cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(user_request, answer):\n",
    "    \n",
    "    conversation = [ {'role': 'user', 'content': user_request},\n",
    "                  {'role': 'assistant', 'content': answer}]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input label\n",
      "65847   Question: Is short Time Interval Between Neuro...    no\n",
      "118346  Question: Does universal health insurance cove...    no\n",
      "116901  Question: Is the spectrum of in vitro radiosen...    no\n",
      "51829   Question: Is hyperhomocysteinemia in children ...    no\n",
      "39075   Question: Does mechanical pleurodesis reduce r...    no\n",
      "...                                                   ...   ...\n",
      "125548  Question: Is albumin the major plasma protein ...   yes\n",
      "157294  Question: Does the trans-chromosomic mouse-der...   yes\n",
      "209371  Question: Does typical savings from each minut...   yes\n",
      "91596   Question: Is [ Expression of aquaporin 3 and a...   yes\n",
      "107726  Question: Is urine cortisol concentration as a...   yes\n",
      "\n",
      "[20000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('qiaojin/PubMedQA', 'pqa_artificial')\n",
    "q = dataset['train']['question']\n",
    "c = dataset['train']['context']\n",
    "label = dataset['train']['final_decision']\n",
    "\n",
    "# 'q'와 'c'를 페어링하여 input 열 만들기\n",
    "input_list = [f\"Question: {q_} Context: {c_}\" for q_, c_ in zip(q, c)]\n",
    "\n",
    "\n",
    "df_all = pd.DataFrame({'input': input_list,'label':label})\n",
    "# 'no'인 값 10000개 추출\n",
    "no_df = df_all[df_all['label'] == 'no'].sample(n=10000, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df_all[df_all['label'] == 'yes'].sample(n=10000, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df = pd.concat([no_df, yes_df])\n",
    "\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test 데이터셋 나누기\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_df['input'], combined_df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# train, val 데이터셋 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "del dataset, q, c, label, input_list, df_all, no_df, yes_df, combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6abfa3b4-c0fd-494e-a825-abaca4e5497f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_prompt_list = []\n",
    "for x,y in zip(X_train, y_train):\n",
    "    train_data_prompt_list.append(make_prompt(x,y))\n",
    "\n",
    "valid_data_prompt_list = []\n",
    "for x2,y2 in zip(X_val, y_val):\n",
    "    valid_data_prompt_list.append(make_prompt(x2,y2))\n",
    "\n",
    "test_data_prompt_list = []\n",
    "for x3,y3 in zip(X_test, y_test):\n",
    "    test_data_prompt_list.append(make_prompt(x3,y3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, x2, y2, x3, y3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nQuestion: Is calcipotriol Plus Betamethasone Dipropionate Aerosol Foam Effective , Independent of Body Mass Index and the Extent and Severity of Psoriasis? Context: {\\'contexts\\': [\\'Good treatment adherence is important in the effective management of psoriasis and is related to both the frequency of applications and the amount of product used versus the recommended dose. The efficacy and safety of fixed combination calcipotriol 50\\\\xa0µg/g (Cal) and betamethasone 0.5\\\\xa0mg/g as dipropionate (BD) in the treatment of psoriasis is well established; an aerosol foam formulation has been developed to enhance adherence. This subanalysis from the Phase III PSO-FAST study evaluates the amount of Cal/BD foam used during treatment and the association between the extent and severity of baseline disease.\\', \"Patients (≥18\\\\xa0years) with mild-to-severe body psoriasis were randomized 3:1 to once-daily Cal/BD foam or vehicle. The amount of Cal/BD foam and vehicle used over the 4-week study period was evaluated according to three baseline disease assessments: extent of body surface area (BSA) affected by psoriasis, physician\\'s global assessment of disease severity (PGA) and modified psoriasis area and severity index (mPASI). Treatment success and mPASI75 rates were assessed according to body mass index (BMI) and body weight.\", \\'323 patients were randomized to Cal/BD foam and 103 to vehicle. At week 4, the mean total amount of Cal/BD foam used was 120.8\\\\xa0g (n\\\\xa0=\\\\xa0293), which was similar to the amount of vehicle used (128.9\\\\xa0g; n\\\\xa0=\\\\xa098). The total amount of Cal/BD foam used at week 4 was greater with increasing BSA and increasing severity of baseline PGA and mPASI. Throughout the study, 93.1% of patients in the Cal/BD foam group and 99.0% of patients in the vehicle group missed ≤10% of treatment applications. Treatment success and mPASI75 rates were generally similar when stratified according to BMI and body weight.\\'], \\'labels\\': [\\'BACKGROUND\\', \\'METHODS\\', \\'RESULTS\\'], \\'meshes\\': []}<|end|>\\n<|assistant|>\\nyes<|end|>\\n<|assistant|>\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_prompt_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d05d3a2-d9ee-43f2-9f40-980669abd577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89f92546-0270-4385-b977-f75f3892c640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data_prompt_list)\n",
    "valid_dataset = Dataset(valid_data_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "554587bf-a69f-423d-abf1-7a3cbe77746d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    for i, prompt in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        scaler.update()\n",
    "        print(f\"epoch : {epoch} - step : {i}/{len(loader)} - loss: {loss.item()}\")\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "        del inputs\n",
    "        del outputs\n",
    "        del loss\n",
    "        \n",
    "    print(f'Epoch: {epoch}, train_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Train'].append(loss_avg/len(loader))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd12d81f-d0a5-4694-9971-504645178cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch,loader):  \n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    with torch.no_grad():       \n",
    "        for i, prompt in enumerate(loader):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss_avg += loss.item()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            del loss\n",
    "            \n",
    "    print(f'Epoch: {epoch}, Valid_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Val'].append(loss_avg/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "35f933dc-9b98-42ed-88b4-1cf78e7ed602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "184ee7b8-d2bf-4153-bea4-bd29fc30bd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 3e-4)\n",
    "# optimizer = SGD(model.parameters(), lr=3e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 3e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=227,\n",
    "    num_training_steps=15000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5b4791b-bdab-4cf0-b465-e66be73c1ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 - step : 0/1500 - loss: 3.4250924587249756\n",
      "epoch : 1 - step : 1/1500 - loss: 3.0902509689331055\n",
      "epoch : 1 - step : 2/1500 - loss: 4.341043949127197\n",
      "epoch : 1 - step : 3/1500 - loss: 3.307300090789795\n",
      "epoch : 1 - step : 4/1500 - loss: 4.039970874786377\n",
      "epoch : 1 - step : 5/1500 - loss: 3.607780933380127\n",
      "epoch : 1 - step : 6/1500 - loss: 3.518313407897949\n",
      "epoch : 1 - step : 7/1500 - loss: 4.145331382751465\n",
      "epoch : 1 - step : 8/1500 - loss: 2.919867753982544\n",
      "epoch : 1 - step : 9/1500 - loss: 3.853266477584839\n",
      "epoch : 1 - step : 10/1500 - loss: 2.9841010570526123\n",
      "epoch : 1 - step : 11/1500 - loss: 2.738149404525757\n",
      "epoch : 1 - step : 12/1500 - loss: 3.3627140522003174\n",
      "epoch : 1 - step : 13/1500 - loss: 3.3168561458587646\n",
      "epoch : 1 - step : 14/1500 - loss: 3.3926730155944824\n",
      "epoch : 1 - step : 15/1500 - loss: 2.9461305141448975\n",
      "epoch : 1 - step : 16/1500 - loss: 3.076738119125366\n",
      "epoch : 1 - step : 17/1500 - loss: 3.1123063564300537\n",
      "epoch : 1 - step : 18/1500 - loss: 3.3475465774536133\n",
      "epoch : 1 - step : 19/1500 - loss: 2.2561872005462646\n",
      "epoch : 1 - step : 20/1500 - loss: 3.9784743785858154\n",
      "epoch : 1 - step : 21/1500 - loss: 3.3881185054779053\n",
      "epoch : 1 - step : 22/1500 - loss: 3.994821310043335\n",
      "epoch : 1 - step : 23/1500 - loss: 3.706146240234375\n",
      "epoch : 1 - step : 24/1500 - loss: 3.261103868484497\n",
      "epoch : 1 - step : 25/1500 - loss: 3.2347378730773926\n",
      "epoch : 1 - step : 26/1500 - loss: 3.7547309398651123\n",
      "epoch : 1 - step : 27/1500 - loss: 3.5551187992095947\n",
      "epoch : 1 - step : 28/1500 - loss: 3.5920326709747314\n",
      "epoch : 1 - step : 29/1500 - loss: 3.130126476287842\n",
      "epoch : 1 - step : 30/1500 - loss: 3.9588234424591064\n",
      "epoch : 1 - step : 31/1500 - loss: 3.7301101684570312\n",
      "epoch : 1 - step : 32/1500 - loss: 3.977714776992798\n",
      "epoch : 1 - step : 33/1500 - loss: 3.218997001647949\n",
      "epoch : 1 - step : 34/1500 - loss: 3.4249494075775146\n",
      "epoch : 1 - step : 35/1500 - loss: 3.5386571884155273\n",
      "epoch : 1 - step : 36/1500 - loss: 3.177140235900879\n",
      "epoch : 1 - step : 37/1500 - loss: 3.850567102432251\n",
      "epoch : 1 - step : 38/1500 - loss: 3.372174024581909\n",
      "epoch : 1 - step : 39/1500 - loss: 2.5780584812164307\n",
      "epoch : 1 - step : 40/1500 - loss: 3.56978178024292\n",
      "epoch : 1 - step : 41/1500 - loss: 2.9457452297210693\n",
      "epoch : 1 - step : 42/1500 - loss: 4.276013374328613\n",
      "epoch : 1 - step : 43/1500 - loss: 3.2071120738983154\n",
      "epoch : 1 - step : 44/1500 - loss: 3.2051162719726562\n",
      "epoch : 1 - step : 45/1500 - loss: 3.3366758823394775\n",
      "epoch : 1 - step : 46/1500 - loss: 3.562565803527832\n",
      "epoch : 1 - step : 47/1500 - loss: 4.372076034545898\n",
      "epoch : 1 - step : 48/1500 - loss: 3.3100533485412598\n",
      "epoch : 1 - step : 49/1500 - loss: 4.593538284301758\n",
      "epoch : 1 - step : 50/1500 - loss: 3.4843063354492188\n",
      "epoch : 1 - step : 51/1500 - loss: 2.7655813694000244\n",
      "epoch : 1 - step : 52/1500 - loss: 3.999964475631714\n",
      "epoch : 1 - step : 53/1500 - loss: 3.9884390830993652\n",
      "epoch : 1 - step : 54/1500 - loss: 3.2454278469085693\n",
      "epoch : 1 - step : 55/1500 - loss: 3.5802462100982666\n",
      "epoch : 1 - step : 56/1500 - loss: 3.7965261936187744\n",
      "epoch : 1 - step : 57/1500 - loss: 3.7785842418670654\n",
      "epoch : 1 - step : 58/1500 - loss: 2.9496383666992188\n",
      "epoch : 1 - step : 59/1500 - loss: 4.129331588745117\n",
      "epoch : 1 - step : 60/1500 - loss: 4.276776313781738\n",
      "epoch : 1 - step : 61/1500 - loss: 4.2188568115234375\n",
      "epoch : 1 - step : 62/1500 - loss: 3.3612635135650635\n",
      "epoch : 1 - step : 63/1500 - loss: 5.094809532165527\n",
      "epoch : 1 - step : 64/1500 - loss: 2.397724151611328\n",
      "epoch : 1 - step : 65/1500 - loss: 2.988441228866577\n",
      "epoch : 1 - step : 66/1500 - loss: 3.020965814590454\n",
      "epoch : 1 - step : 67/1500 - loss: 3.3952221870422363\n",
      "epoch : 1 - step : 68/1500 - loss: 3.2319867610931396\n",
      "epoch : 1 - step : 69/1500 - loss: 3.661600112915039\n",
      "epoch : 1 - step : 70/1500 - loss: 2.980518341064453\n",
      "epoch : 1 - step : 71/1500 - loss: 3.2629776000976562\n",
      "epoch : 1 - step : 72/1500 - loss: 4.012057304382324\n",
      "epoch : 1 - step : 73/1500 - loss: 4.300692558288574\n",
      "epoch : 1 - step : 74/1500 - loss: 4.259547233581543\n",
      "epoch : 1 - step : 75/1500 - loss: 3.260436773300171\n",
      "epoch : 1 - step : 76/1500 - loss: 3.55822491645813\n",
      "epoch : 1 - step : 77/1500 - loss: 2.578594446182251\n",
      "epoch : 1 - step : 78/1500 - loss: 3.7283453941345215\n",
      "epoch : 1 - step : 79/1500 - loss: 3.088933229446411\n",
      "epoch : 1 - step : 80/1500 - loss: 2.7081034183502197\n",
      "epoch : 1 - step : 81/1500 - loss: 5.208765506744385\n",
      "epoch : 1 - step : 82/1500 - loss: 4.740739822387695\n",
      "epoch : 1 - step : 83/1500 - loss: 2.7837119102478027\n",
      "epoch : 1 - step : 84/1500 - loss: 3.482293128967285\n",
      "epoch : 1 - step : 85/1500 - loss: 4.105628967285156\n",
      "epoch : 1 - step : 86/1500 - loss: 4.449869155883789\n",
      "epoch : 1 - step : 87/1500 - loss: 3.543836832046509\n",
      "epoch : 1 - step : 88/1500 - loss: 4.138556480407715\n",
      "epoch : 1 - step : 89/1500 - loss: 3.4136159420013428\n",
      "epoch : 1 - step : 90/1500 - loss: 3.755424976348877\n",
      "epoch : 1 - step : 91/1500 - loss: 4.691494941711426\n",
      "epoch : 1 - step : 92/1500 - loss: 2.8333754539489746\n",
      "epoch : 1 - step : 93/1500 - loss: 3.3856379985809326\n",
      "epoch : 1 - step : 94/1500 - loss: 3.205859661102295\n",
      "epoch : 1 - step : 95/1500 - loss: 4.140777587890625\n",
      "epoch : 1 - step : 96/1500 - loss: 3.64492130279541\n",
      "epoch : 1 - step : 97/1500 - loss: 2.9795475006103516\n",
      "epoch : 1 - step : 98/1500 - loss: 3.0970051288604736\n",
      "epoch : 1 - step : 99/1500 - loss: 2.859590530395508\n",
      "epoch : 1 - step : 100/1500 - loss: 3.7492053508758545\n",
      "epoch : 1 - step : 101/1500 - loss: 3.196669340133667\n",
      "epoch : 1 - step : 102/1500 - loss: 3.307065725326538\n",
      "epoch : 1 - step : 103/1500 - loss: 2.937345027923584\n",
      "epoch : 1 - step : 104/1500 - loss: 3.1179332733154297\n",
      "epoch : 1 - step : 105/1500 - loss: 4.064330101013184\n",
      "epoch : 1 - step : 106/1500 - loss: 3.4338860511779785\n",
      "epoch : 1 - step : 107/1500 - loss: 3.338338851928711\n",
      "epoch : 1 - step : 108/1500 - loss: 3.221890449523926\n",
      "epoch : 1 - step : 109/1500 - loss: 2.751279830932617\n",
      "epoch : 1 - step : 110/1500 - loss: 3.6422739028930664\n",
      "epoch : 1 - step : 111/1500 - loss: 3.0731635093688965\n",
      "epoch : 1 - step : 112/1500 - loss: 4.02816104888916\n",
      "epoch : 1 - step : 113/1500 - loss: 2.9870786666870117\n",
      "epoch : 1 - step : 114/1500 - loss: 3.128185749053955\n",
      "epoch : 1 - step : 115/1500 - loss: 3.3960812091827393\n",
      "epoch : 1 - step : 116/1500 - loss: 2.9761083126068115\n",
      "epoch : 1 - step : 117/1500 - loss: 4.146066188812256\n",
      "epoch : 1 - step : 118/1500 - loss: 3.7099061012268066\n",
      "epoch : 1 - step : 119/1500 - loss: 4.391618728637695\n",
      "epoch : 1 - step : 120/1500 - loss: 3.4289140701293945\n",
      "epoch : 1 - step : 121/1500 - loss: 3.699209451675415\n",
      "epoch : 1 - step : 122/1500 - loss: 2.5705337524414062\n",
      "epoch : 1 - step : 123/1500 - loss: 3.3842549324035645\n",
      "epoch : 1 - step : 124/1500 - loss: 2.746438980102539\n",
      "epoch : 1 - step : 125/1500 - loss: 3.289600372314453\n",
      "epoch : 1 - step : 126/1500 - loss: 4.041276931762695\n",
      "epoch : 1 - step : 127/1500 - loss: 3.9974780082702637\n",
      "epoch : 1 - step : 128/1500 - loss: 3.3727588653564453\n",
      "epoch : 1 - step : 129/1500 - loss: 3.0423827171325684\n",
      "epoch : 1 - step : 130/1500 - loss: 2.9339406490325928\n",
      "epoch : 1 - step : 131/1500 - loss: 3.1170668601989746\n",
      "epoch : 1 - step : 132/1500 - loss: 3.425633430480957\n",
      "epoch : 1 - step : 133/1500 - loss: 2.9927144050598145\n",
      "epoch : 1 - step : 134/1500 - loss: 3.4510068893432617\n",
      "epoch : 1 - step : 135/1500 - loss: 3.428197145462036\n",
      "epoch : 1 - step : 136/1500 - loss: 4.328804969787598\n",
      "epoch : 1 - step : 137/1500 - loss: 3.5672478675842285\n",
      "epoch : 1 - step : 138/1500 - loss: 4.126067161560059\n",
      "epoch : 1 - step : 139/1500 - loss: 2.782808780670166\n",
      "epoch : 1 - step : 140/1500 - loss: 3.4659409523010254\n",
      "epoch : 1 - step : 141/1500 - loss: 2.839852809906006\n",
      "epoch : 1 - step : 142/1500 - loss: 3.769379138946533\n",
      "epoch : 1 - step : 143/1500 - loss: 3.5147998332977295\n",
      "epoch : 1 - step : 144/1500 - loss: 3.1190361976623535\n",
      "epoch : 1 - step : 145/1500 - loss: 3.8456997871398926\n",
      "epoch : 1 - step : 146/1500 - loss: 3.4520277976989746\n",
      "epoch : 1 - step : 147/1500 - loss: 3.648109197616577\n",
      "epoch : 1 - step : 148/1500 - loss: 3.0642900466918945\n",
      "epoch : 1 - step : 149/1500 - loss: 3.4727916717529297\n",
      "epoch : 1 - step : 150/1500 - loss: 3.9947400093078613\n",
      "epoch : 1 - step : 151/1500 - loss: 4.12470817565918\n",
      "epoch : 1 - step : 152/1500 - loss: 3.0766842365264893\n",
      "epoch : 1 - step : 153/1500 - loss: 2.9255237579345703\n",
      "epoch : 1 - step : 154/1500 - loss: 3.1390111446380615\n",
      "epoch : 1 - step : 155/1500 - loss: 4.019285678863525\n",
      "epoch : 1 - step : 156/1500 - loss: 3.450897455215454\n",
      "epoch : 1 - step : 157/1500 - loss: 3.3032724857330322\n",
      "epoch : 1 - step : 158/1500 - loss: 3.3277461528778076\n",
      "epoch : 1 - step : 159/1500 - loss: 4.172623634338379\n",
      "epoch : 1 - step : 160/1500 - loss: 4.752795219421387\n",
      "epoch : 1 - step : 161/1500 - loss: 2.8566455841064453\n",
      "epoch : 1 - step : 162/1500 - loss: 3.6522881984710693\n",
      "epoch : 1 - step : 163/1500 - loss: 3.6153130531311035\n",
      "epoch : 1 - step : 164/1500 - loss: 3.673938751220703\n",
      "epoch : 1 - step : 165/1500 - loss: 3.921398162841797\n",
      "epoch : 1 - step : 166/1500 - loss: 3.6155877113342285\n",
      "epoch : 1 - step : 167/1500 - loss: 3.7677838802337646\n",
      "epoch : 1 - step : 168/1500 - loss: 2.9929399490356445\n",
      "epoch : 1 - step : 169/1500 - loss: 4.000293254852295\n",
      "epoch : 1 - step : 170/1500 - loss: 3.7925450801849365\n",
      "epoch : 1 - step : 171/1500 - loss: 2.430389881134033\n",
      "epoch : 1 - step : 172/1500 - loss: 4.498615264892578\n",
      "epoch : 1 - step : 173/1500 - loss: 3.787121057510376\n",
      "epoch : 1 - step : 174/1500 - loss: 3.51426100730896\n",
      "epoch : 1 - step : 175/1500 - loss: 3.583587408065796\n",
      "epoch : 1 - step : 176/1500 - loss: 3.8233656883239746\n",
      "epoch : 1 - step : 177/1500 - loss: 3.3490960597991943\n",
      "epoch : 1 - step : 178/1500 - loss: 2.2499876022338867\n",
      "epoch : 1 - step : 179/1500 - loss: 3.205648183822632\n",
      "epoch : 1 - step : 180/1500 - loss: 3.810488224029541\n",
      "epoch : 1 - step : 181/1500 - loss: 3.775817632675171\n",
      "epoch : 1 - step : 182/1500 - loss: 3.5341482162475586\n",
      "epoch : 1 - step : 183/1500 - loss: 3.004354238510132\n",
      "epoch : 1 - step : 184/1500 - loss: 3.860410213470459\n",
      "epoch : 1 - step : 185/1500 - loss: 3.3148956298828125\n",
      "epoch : 1 - step : 186/1500 - loss: 3.916626214981079\n",
      "epoch : 1 - step : 187/1500 - loss: 3.2709147930145264\n",
      "epoch : 1 - step : 188/1500 - loss: 3.44804048538208\n",
      "epoch : 1 - step : 189/1500 - loss: 4.639955997467041\n",
      "epoch : 1 - step : 190/1500 - loss: 3.41540265083313\n",
      "epoch : 1 - step : 191/1500 - loss: 4.164510250091553\n",
      "epoch : 1 - step : 192/1500 - loss: 3.5493788719177246\n",
      "epoch : 1 - step : 193/1500 - loss: 3.3220372200012207\n",
      "epoch : 1 - step : 194/1500 - loss: 4.039734840393066\n",
      "epoch : 1 - step : 195/1500 - loss: 4.392388343811035\n",
      "epoch : 1 - step : 196/1500 - loss: 3.0739972591400146\n",
      "epoch : 1 - step : 197/1500 - loss: 4.092816352844238\n",
      "epoch : 1 - step : 198/1500 - loss: 3.7076070308685303\n",
      "epoch : 1 - step : 199/1500 - loss: 4.235783576965332\n",
      "epoch : 1 - step : 200/1500 - loss: 3.405468463897705\n",
      "epoch : 1 - step : 201/1500 - loss: 2.4871022701263428\n",
      "epoch : 1 - step : 202/1500 - loss: 3.993764638900757\n",
      "epoch : 1 - step : 203/1500 - loss: 3.797603130340576\n",
      "epoch : 1 - step : 204/1500 - loss: 3.2798573970794678\n",
      "epoch : 1 - step : 205/1500 - loss: 4.335248947143555\n",
      "epoch : 1 - step : 206/1500 - loss: 3.9420011043548584\n",
      "epoch : 1 - step : 207/1500 - loss: 3.5176007747650146\n",
      "epoch : 1 - step : 208/1500 - loss: 4.16750431060791\n",
      "epoch : 1 - step : 209/1500 - loss: 3.3559954166412354\n",
      "epoch : 1 - step : 210/1500 - loss: 3.689833402633667\n",
      "epoch : 1 - step : 211/1500 - loss: 3.3592581748962402\n",
      "epoch : 1 - step : 212/1500 - loss: 3.7342278957366943\n",
      "epoch : 1 - step : 213/1500 - loss: 3.344982862472534\n",
      "epoch : 1 - step : 214/1500 - loss: 3.335714817047119\n",
      "epoch : 1 - step : 215/1500 - loss: 2.828169345855713\n",
      "epoch : 1 - step : 216/1500 - loss: 3.698848247528076\n",
      "epoch : 1 - step : 217/1500 - loss: 2.3253002166748047\n",
      "epoch : 1 - step : 218/1500 - loss: 3.676539897918701\n",
      "epoch : 1 - step : 219/1500 - loss: 2.899320602416992\n",
      "epoch : 1 - step : 220/1500 - loss: 3.751577854156494\n",
      "epoch : 1 - step : 221/1500 - loss: 3.1615872383117676\n",
      "epoch : 1 - step : 222/1500 - loss: 4.2126145362854\n",
      "epoch : 1 - step : 223/1500 - loss: 3.4192183017730713\n",
      "epoch : 1 - step : 224/1500 - loss: 3.464301586151123\n",
      "epoch : 1 - step : 225/1500 - loss: 2.9056715965270996\n",
      "epoch : 1 - step : 226/1500 - loss: 5.019704818725586\n",
      "epoch : 1 - step : 227/1500 - loss: 4.226658344268799\n",
      "epoch : 1 - step : 228/1500 - loss: 2.806234359741211\n",
      "epoch : 1 - step : 229/1500 - loss: 2.9416537284851074\n",
      "epoch : 1 - step : 230/1500 - loss: 3.570188522338867\n",
      "epoch : 1 - step : 231/1500 - loss: 3.1012184619903564\n",
      "epoch : 1 - step : 232/1500 - loss: 3.2798562049865723\n",
      "epoch : 1 - step : 233/1500 - loss: 3.4800755977630615\n",
      "epoch : 1 - step : 234/1500 - loss: 3.4806203842163086\n",
      "epoch : 1 - step : 235/1500 - loss: 3.616368055343628\n",
      "epoch : 1 - step : 236/1500 - loss: 3.230278730392456\n",
      "epoch : 1 - step : 237/1500 - loss: 4.336121559143066\n",
      "epoch : 1 - step : 238/1500 - loss: 2.9827654361724854\n",
      "epoch : 1 - step : 239/1500 - loss: 3.186668634414673\n",
      "epoch : 1 - step : 240/1500 - loss: 2.937431573867798\n",
      "epoch : 1 - step : 241/1500 - loss: 3.240936517715454\n",
      "epoch : 1 - step : 242/1500 - loss: 3.2885146141052246\n",
      "epoch : 1 - step : 243/1500 - loss: 3.2569382190704346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [14:35<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/work/LLM-CL/temp.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m)):\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     loss_dic[\u001b[39m'\u001b[39m\u001b[39mepoch\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(epoch)\n\u001b[0;32m---> <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     train(epoch, train_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     validate(epoch, valid_loader)\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "\u001b[1;32m/home/work/LLM-CL/temp.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs, labels\u001b[39m=\u001b[39minputs[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m scaler\u001b[39m.\u001b[39;49mscale(loss)\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m scaler\u001b[39m.\u001b[39mstep(optimizer)\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m#loss.backward()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://proxy2.aitrain.ktcloud.com:10695/home/work/LLM-CL/temp.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m#optimizer.step()\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/main/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    526\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    527\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/main/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m _engine_run_backward(\n\u001b[1;32m    268\u001b[0m     tensors,\n\u001b[1;32m    269\u001b[0m     grad_tensors_,\n\u001b[1;32m    270\u001b[0m     retain_graph,\n\u001b[1;32m    271\u001b[0m     create_graph,\n\u001b[1;32m    272\u001b[0m     inputs,\n\u001b[1;32m    273\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    275\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/main/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[39m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m         t_outputs, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    746\u001b[0m     )  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "loss_dic = {\"epoch\":[],\"Train\":[], \"Val\":[]}\n",
    "best_loss = 100\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in tqdm(range(1, 10)):\n",
    "    \n",
    "    loss_dic['epoch'].append(epoch)\n",
    "    train(epoch, train_loader)\n",
    "    validate(epoch, valid_loader)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if loss_dic['Val'][epoch - 1] > best_loss:\n",
    "        early_stop_count += 1       \n",
    "        if early_stop_count >= 2:\n",
    "            loss_dic_df = pd.DataFrame(loss_dic)\n",
    "            # loss_dic_df.to_excel('./loss.xlsx', index=False)\n",
    "            # torch.save(model.state_dict(), f'./bestmodel_{epoch}.pth')\n",
    "            break\n",
    "    else:\n",
    "        best_loss = loss_dic['Val'][epoch - 1]\n",
    "        early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbfb2c-2f94-4de8-a3bb-a7bfc2f514a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
