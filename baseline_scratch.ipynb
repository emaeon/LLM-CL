{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d9d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9267e0-1905-4e0a-90b0-63e7813128b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig,HfArgumentParser, get_scheduler, set_seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53ef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'mode_ID':\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "          'seed': 1 ,\n",
    "          'max_seq_len' : 4096,\n",
    "          'epochs': 3,\n",
    "          'lr': 2e-4,\n",
    "          'batch': 4,\n",
    "          'lora_r':8,\n",
    "          'lora_alpha':32,\n",
    "          'target_module':[\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\",\"gate_proj\", \"v_proj\"],\n",
    "          'lora_dropout':0.05,\n",
    "          'lora_tasktype' :'CAUSAL_LM',\n",
    "          'lora_bias' : 'none',\n",
    "          'optimizer': 'paged_adamw_8bit',\n",
    "          'scheduler':'cosine'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516cc735",
   "metadata": {},
   "source": [
    "## Model 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a76dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,  # PEFT 설정을 가져오기 위한 함수\n",
    "    get_peft_model,  # PEFT 모델을 가져오기 위한 함수\n",
    "    get_peft_model_state_dict,  # PEFT 모델 상태 사전을 가져오기 위한 함수\n",
    "    set_peft_model_state_dict,  # PEFT 모델 상태 사전을 설정하기 위한 함수\n",
    "    LoraConfig,  # LoRA 모델 구성을 정의하는 클래스\n",
    "    PeftType,  # PEFT 모델의 타입을 정의\n",
    "    PrefixTuningConfig,  # PrefixTuning 모델 구성을 정의하는 클래스\n",
    "    PromptEncoderConfig,  # PromptEncoder 모델 구성을 정의하는 클래스\n",
    "    PeftModel,  # PEFT 모델을 정의하는 클래스\n",
    "    PeftConfig,  # PEFT 모델의 구성을 정의하는 클래스\n",
    ")\n",
    "\n",
    "# PEFT 모델의 타입 설정 (LoRA로 설정)\n",
    "peft_type = PeftType.LORA\n",
    "\n",
    "# LoRA 모델을 위한 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=config['lora_r'],  # LoRA 모델의 r 값\n",
    "    lora_alpha=config['lora_alpha'],  # LoRA 모델의 alpha 값\n",
    "    target_modules=config['target_module'],  # LoRA 모델의 타겟 모듈 리스트\n",
    "    lora_dropout=config['lora_dropout'],  # LoRA 모델의 드롭아웃 비율\n",
    "    bias=config['lora_bias'],  # LoRA 모델의 편향 설정\n",
    "    task_type=config['lora_tasktype']  # LoRA 모델의 태스크 유형\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed05f2fd-0929-4c62-8b7e-2c26eb3b0b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.72s/it]\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer를 사용하여 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['mode_ID'], trust_remote_code=True, eos_token='</s>')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig['mode_ID'],\n",
    "\tdevice_map=\"cuda\",\n",
    "\ttorch_dtype=torch.float16,\n",
    "\ttrust_remote_code=True, \n",
    "\tuse_cache=False,\n",
    "    # attn_implementation='flash_attention_2'\n",
    "\t# quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable() # 모델에서 그래디언트 체크포인팅 활성화 (메모리 효율 향상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ba1119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3 크기 : 3821.1M개의 파라미터\n"
     ]
    }
   ],
   "source": [
    "print(f'Phi3 크기 : {model.num_parameters()/1000**2:.1f}M개의 파라미터')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b199022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.1165\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training # peft 라이브러리에서 k 비트 학습 준비 함수 임포트\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # CUDA 사용 가능 여부 확인\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)# k 비트 학습을 위해 모델 준비 - prepare_model_for_kbit_training 함수 사용\n",
    "model = get_peft_model(model, peft_config) # PEFT 적용 \n",
    "model = model.to(device) # 모델을 학습 장치 (GPU 등)로 이동\n",
    "model.print_trainable_parameters()# 훈련 가능한 파라미터 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23758421-c94e-463a-a25b-048854782cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(user_request, answer):\n",
    "    \n",
    "    conversation = [ {'role': 'user', 'content': user_request},\n",
    "                  {'role': 'assistant', 'content': answer}]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bd1bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input final_decision\n",
      "133561  Question:\\nDoes aspirin increase bleeding comp...             no\n",
      "123846  Question:\\nAre measures of socioeconomic posit...             no\n",
      "143951  Question:\\nDoes dialysis within 24 hours of tr...             no\n",
      "79644   Question:\\nIs mild renal pelvic dilatation pre...             no\n",
      "108150  Question:\\nDoes acute blood pressure reduction...             no\n",
      "...                                                   ...            ...\n",
      "165729  Question:\\nDoes cytoplasmic maspin expression ...            yes\n",
      "75155   Question:\\nDo layer-shaped alginate hydrogels ...            yes\n",
      "193747  Question:\\nDoes oral administration of green p...            yes\n",
      "34532   Question:\\nAre mast cells involved in the path...            yes\n",
      "104153  Question:\\nDoes a patient information booklet ...            yes\n",
      "\n",
      "[1000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "with open('./data/pqaa_train_set.json','r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('./data/pqaa_dev_set.json','r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "# 데이터프레임에 넣을 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 딕셔너리를 순회하며 데이터프레임용 리스트 생성\n",
    "for num, details in train_data.items():\n",
    "    contexts_with_labels = '\\n'.join([f\"({label}) {context}\" for label, context in zip(details['LABELS'], details['CONTEXTS'])])\n",
    "    input = 'Question:\\n' + details['QUESTION'] + '\\nPlease give me the answer in formats: yes or no' + '\\n' + 'Context:\\n' + contexts_with_labels\n",
    "    row = {\n",
    "        'input' : input,\n",
    "        'final_decision': details['final_decision']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=500, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=500, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df = pd.concat([no_df, yes_df])\n",
    "\n",
    "print(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b255989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   input final_decision\n",
      "2774   Question:\\nDoes trimetazidine modify blood lev...             no\n",
      "4083   Question:\\nDo aDAMTS-5 deficient mice develop ...             no\n",
      "10063  Question:\\nIs brucellosis a major cause of feb...             no\n",
      "3060   Question:\\nDoes sertraline alter the beta-adre...             no\n",
      "7219   Question:\\nAre salivary biomarkers suitable fo...             no\n",
      "...                                                  ...            ...\n",
      "1469   Question:\\nDoes varus malalignment negate the ...            yes\n",
      "3216   Question:\\nDo inflammatory protein levels and ...            yes\n",
      "8590   Question:\\nDoes intraaortic balloon pumping im...            yes\n",
      "4770   Question:\\nDo girls ' childhood trajectories o...            yes\n",
      "6134   Question:\\nIs activation of phospholipase A2 a...            yes\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# 데이터프레임에 넣을 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 딕셔너리를 순회하며 데이터프레임용 리스트 생성\n",
    "for num, details in test_data.items():\n",
    "    contexts_with_labels = '\\n'.join([f\"({label}) {context}\" for label, context in zip(details['LABELS'], details['CONTEXTS'])])\n",
    "    input = 'Question:\\n' + details['QUESTION'] + '\\nPlease give me the answer in formats: yes or no' + '\\n' + 'Context:\\n' + contexts_with_labels\n",
    "    row = {\n",
    "        'input' : input,\n",
    "        'final_decision': details['final_decision']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=50, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=50, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df_test = pd.concat([no_df, yes_df])\n",
    "\n",
    "print(combined_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2f0ede08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "final_decision\n",
       "no     500\n",
       "yes    500\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df['final_decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbff1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, valid 데이터셋 나누기\n",
    "X_train, X_val, y_train, y_val = train_test_split(combined_df['input'], combined_df['final_decision'], test_size=0.2, random_state=42)\n",
    "\n",
    "# test 데이터셋\n",
    "X_test = combined_df_test['input']\n",
    "y_test = combined_df_test['final_decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba23a1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "del no_df, yes_df, combined_df, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abfa3b4-c0fd-494e-a825-abaca4e5497f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_prompt_list = []\n",
    "for x,y in zip(X_train, y_train):\n",
    "    train_data_prompt_list.append(make_prompt(x,y))\n",
    "\n",
    "valid_data_prompt_list = []\n",
    "for x2,y2 in zip(X_val, y_val):\n",
    "    valid_data_prompt_list.append(make_prompt(x2,y2))\n",
    "\n",
    "test_data_prompt_list = []\n",
    "for x3,y3 in zip(X_test, y_test):\n",
    "    test_data_prompt_list.append(make_prompt(x3,y3))\n",
    "    test_data_prompt_list = [test_data.split('<|end|>')[0] + '<|end|>' for test_data in test_data_prompt_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "421e6931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nQuestion:\\nDoes trimetazidine modify blood levels and immunosuppressant effects of cyclosporine A in renal allograft recipients?\\nPlease give me the answer in formats: yes or no\\nContext:\\n(OBJECTIVE) In renal allograft recipients, trimetazidine (Vastarel) was proposed to be associated with the classic immunosuppressant treatments because it displays anti-ischaemic effects which may protect against cyclosporine A nephrotoxicity. The objective of this work was to assess the possibility of coadministering cyclosporin A, Sandimmun, and trimetazidine.\\n(METHODS) Twelve renal transplant patients were selected on the basis of the stability of their cyclosporine A blood concentrations for the previous 3 months. They received trimetazidine, 40 mg twice daily orally for 5 days. Other coadministered drugs were kept unchanged during the study. Before and after trimetazidine administration, cyclosporine A blood concentrations, plasma interleukin-2 and soluble interleukin-2 receptor levels were measured.\\n(RESULTS) The data showed that neither cyclosporin A blood pharmacokinetic parameters, Cmax, tmax, AUC, nor the concentrations of interleukin-2 and soluble interleukin-2 receptors were significantly modified.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_prompt_list[0].split('<|end|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54193a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nQuestion:\\nDoes trimetazidine modify blood levels and immunosuppressant effects of cyclosporine A in renal allograft recipients?\\nPlease give me the answer in formats: yes or no\\nContext:\\n(OBJECTIVE) In renal allograft recipients, trimetazidine (Vastarel) was proposed to be associated with the classic immunosuppressant treatments because it displays anti-ischaemic effects which may protect against cyclosporine A nephrotoxicity. The objective of this work was to assess the possibility of coadministering cyclosporin A, Sandimmun, and trimetazidine.\\n(METHODS) Twelve renal transplant patients were selected on the basis of the stability of their cyclosporine A blood concentrations for the previous 3 months. They received trimetazidine, 40 mg twice daily orally for 5 days. Other coadministered drugs were kept unchanged during the study. Before and after trimetazidine administration, cyclosporine A blood concentrations, plasma interleukin-2 and soluble interleukin-2 receptor levels were measured.\\n(RESULTS) The data showed that neither cyclosporin A blood pharmacokinetic parameters, Cmax, tmax, AUC, nor the concentrations of interleukin-2 and soluble interleukin-2 receptors were significantly modified.<|end|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_prompt_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d05d3a2-d9ee-43f2-9f40-980669abd577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f92546-0270-4385-b977-f75f3892c640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data_prompt_list)\n",
    "valid_dataset = Dataset(valid_data_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "554587bf-a69f-423d-abf1-7a3cbe77746d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, loader):\n",
    "\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    for i, prompt in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        scaler.update()\n",
    "        print(f\"epoch : {epoch} - step : {i}/{len(loader)} - loss: {loss.item()}\")\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "        del inputs\n",
    "        del outputs\n",
    "        del loss\n",
    "        \n",
    "    print(f'Epoch: {epoch}, train_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Train'].append(loss_avg/len(loader))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd12d81f-d0a5-4694-9971-504645178cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch,loader):  \n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    with torch.no_grad():       \n",
    "        for i, prompt in enumerate(loader):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss_avg += loss.item()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            del loss\n",
    "            \n",
    "    print(f'Epoch: {epoch}, Valid_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Val'].append(loss_avg/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35f933dc-9b98-42ed-88b4-1cf78e7ed602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "184ee7b8-d2bf-4153-bea4-bd29fc30bd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr = 3e-4)\n",
    "# # optimizer = SGD(model.parameters(), lr=3e-4)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "903278f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = 3e-4)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=227,\n",
    "    num_training_steps=15000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5b4791b-bdab-4cf0-b465-e66be73c1ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 - step : 0/100 - loss: 2.843454599380493\n",
      "epoch : 1 - step : 1/100 - loss: 3.6309053897857666\n",
      "epoch : 1 - step : 2/100 - loss: 2.897318124771118\n",
      "epoch : 1 - step : 3/100 - loss: 5.2232513427734375\n",
      "epoch : 1 - step : 4/100 - loss: 3.7282521724700928\n",
      "epoch : 1 - step : 5/100 - loss: 3.6501128673553467\n",
      "epoch : 1 - step : 6/100 - loss: 3.3229799270629883\n",
      "epoch : 1 - step : 7/100 - loss: 4.086520671844482\n",
      "epoch : 1 - step : 8/100 - loss: 3.3903605937957764\n",
      "epoch : 1 - step : 9/100 - loss: 4.179718494415283\n",
      "epoch : 1 - step : 10/100 - loss: 3.5778720378875732\n",
      "epoch : 1 - step : 11/100 - loss: 3.696044921875\n",
      "epoch : 1 - step : 12/100 - loss: 3.1827173233032227\n",
      "epoch : 1 - step : 13/100 - loss: 3.647125005722046\n",
      "epoch : 1 - step : 14/100 - loss: 3.5673530101776123\n",
      "epoch : 1 - step : 15/100 - loss: 2.90842342376709\n",
      "epoch : 1 - step : 16/100 - loss: 3.7696001529693604\n",
      "epoch : 1 - step : 17/100 - loss: 2.7150142192840576\n",
      "epoch : 1 - step : 18/100 - loss: 3.733722686767578\n",
      "epoch : 1 - step : 19/100 - loss: 3.377455472946167\n",
      "epoch : 1 - step : 20/100 - loss: 2.9961395263671875\n",
      "epoch : 1 - step : 21/100 - loss: 3.1172332763671875\n",
      "epoch : 1 - step : 22/100 - loss: 3.916110038757324\n",
      "epoch : 1 - step : 23/100 - loss: 4.0422139167785645\n",
      "epoch : 1 - step : 24/100 - loss: 3.8229966163635254\n",
      "epoch : 1 - step : 25/100 - loss: 2.8140854835510254\n",
      "epoch : 1 - step : 26/100 - loss: 3.4602019786834717\n",
      "epoch : 1 - step : 27/100 - loss: 4.115395545959473\n",
      "epoch : 1 - step : 28/100 - loss: 3.5543441772460938\n",
      "epoch : 1 - step : 29/100 - loss: 2.779846429824829\n",
      "epoch : 1 - step : 30/100 - loss: 3.6391890048980713\n",
      "epoch : 1 - step : 31/100 - loss: 2.8380234241485596\n",
      "epoch : 1 - step : 32/100 - loss: 3.25457763671875\n",
      "epoch : 1 - step : 33/100 - loss: 4.127216339111328\n",
      "epoch : 1 - step : 34/100 - loss: 3.6215293407440186\n",
      "epoch : 1 - step : 35/100 - loss: 3.53398060798645\n",
      "epoch : 1 - step : 36/100 - loss: 3.8017401695251465\n",
      "epoch : 1 - step : 37/100 - loss: 2.8294148445129395\n",
      "epoch : 1 - step : 38/100 - loss: 2.9241480827331543\n",
      "epoch : 1 - step : 39/100 - loss: 3.197524309158325\n",
      "epoch : 1 - step : 40/100 - loss: 3.4234583377838135\n",
      "epoch : 1 - step : 41/100 - loss: 3.3839004039764404\n",
      "epoch : 1 - step : 42/100 - loss: 3.2804863452911377\n",
      "epoch : 1 - step : 43/100 - loss: 4.6653971672058105\n",
      "epoch : 1 - step : 44/100 - loss: 3.3153109550476074\n",
      "epoch : 1 - step : 45/100 - loss: 4.844730377197266\n",
      "epoch : 1 - step : 46/100 - loss: 3.3400766849517822\n",
      "epoch : 1 - step : 47/100 - loss: 4.145996570587158\n",
      "epoch : 1 - step : 48/100 - loss: 3.5546951293945312\n",
      "epoch : 1 - step : 49/100 - loss: 4.178373336791992\n",
      "epoch : 1 - step : 50/100 - loss: 3.112672805786133\n",
      "epoch : 1 - step : 51/100 - loss: 3.46288800239563\n",
      "epoch : 1 - step : 52/100 - loss: 3.372298002243042\n",
      "epoch : 1 - step : 53/100 - loss: 2.439993381500244\n",
      "epoch : 1 - step : 54/100 - loss: 3.822537422180176\n",
      "epoch : 1 - step : 55/100 - loss: 3.24674129486084\n",
      "epoch : 1 - step : 56/100 - loss: 3.4655632972717285\n",
      "epoch : 1 - step : 57/100 - loss: 3.488295555114746\n",
      "epoch : 1 - step : 58/100 - loss: 3.5707600116729736\n",
      "epoch : 1 - step : 59/100 - loss: 2.925638437271118\n",
      "epoch : 1 - step : 60/100 - loss: 3.2558932304382324\n",
      "epoch : 1 - step : 61/100 - loss: 3.1797139644622803\n",
      "epoch : 1 - step : 62/100 - loss: 3.0365898609161377\n",
      "epoch : 1 - step : 63/100 - loss: 3.8344192504882812\n",
      "epoch : 1 - step : 64/100 - loss: 2.740074872970581\n",
      "epoch : 1 - step : 65/100 - loss: 3.337679624557495\n",
      "epoch : 1 - step : 66/100 - loss: 3.1653378009796143\n",
      "epoch : 1 - step : 67/100 - loss: 3.2364485263824463\n",
      "epoch : 1 - step : 68/100 - loss: 4.753546237945557\n",
      "epoch : 1 - step : 69/100 - loss: 3.0295448303222656\n",
      "epoch : 1 - step : 70/100 - loss: 4.428499698638916\n",
      "epoch : 1 - step : 71/100 - loss: 4.003944396972656\n",
      "epoch : 1 - step : 72/100 - loss: 3.6692681312561035\n",
      "epoch : 1 - step : 73/100 - loss: 3.3569016456604004\n",
      "epoch : 1 - step : 74/100 - loss: 3.7938733100891113\n",
      "epoch : 1 - step : 75/100 - loss: 3.8940248489379883\n",
      "epoch : 1 - step : 76/100 - loss: 3.3619272708892822\n",
      "epoch : 1 - step : 77/100 - loss: 3.4967193603515625\n",
      "epoch : 1 - step : 78/100 - loss: 5.060512065887451\n",
      "epoch : 1 - step : 79/100 - loss: 4.732662677764893\n",
      "epoch : 1 - step : 80/100 - loss: 3.593320608139038\n",
      "epoch : 1 - step : 81/100 - loss: 2.885826826095581\n",
      "epoch : 1 - step : 82/100 - loss: 3.310417652130127\n",
      "epoch : 1 - step : 83/100 - loss: 2.6155378818511963\n",
      "epoch : 1 - step : 84/100 - loss: 3.218555450439453\n",
      "epoch : 1 - step : 85/100 - loss: 3.1516823768615723\n",
      "epoch : 1 - step : 86/100 - loss: 2.6055996417999268\n",
      "epoch : 1 - step : 87/100 - loss: 3.451833486557007\n",
      "epoch : 1 - step : 88/100 - loss: 2.970899820327759\n",
      "epoch : 1 - step : 89/100 - loss: 3.675682306289673\n",
      "epoch : 1 - step : 90/100 - loss: 3.1142969131469727\n",
      "epoch : 1 - step : 91/100 - loss: 4.298191070556641\n",
      "epoch : 1 - step : 92/100 - loss: 3.183781862258911\n",
      "epoch : 1 - step : 93/100 - loss: 4.775832176208496\n",
      "epoch : 1 - step : 94/100 - loss: 3.0843708515167236\n",
      "epoch : 1 - step : 95/100 - loss: 3.8500571250915527\n",
      "epoch : 1 - step : 96/100 - loss: 4.375640869140625\n",
      "epoch : 1 - step : 97/100 - loss: 3.217744827270508\n",
      "epoch : 1 - step : 98/100 - loss: 4.215143203735352\n",
      "epoch : 1 - step : 99/100 - loss: 3.0914249420166016\n",
      "Epoch: 1, train_Loss:  3.5260734820365904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [06:11<55:42, 371.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Valid_Loss:  3.3627640533447267\n",
      "epoch : 2 - step : 0/100 - loss: 3.908290147781372\n",
      "epoch : 2 - step : 1/100 - loss: 4.468196392059326\n",
      "epoch : 2 - step : 2/100 - loss: 4.456459999084473\n",
      "epoch : 2 - step : 3/100 - loss: 3.8397469520568848\n",
      "epoch : 2 - step : 4/100 - loss: 3.140565872192383\n",
      "epoch : 2 - step : 5/100 - loss: 3.584003448486328\n",
      "epoch : 2 - step : 6/100 - loss: 3.4396822452545166\n",
      "epoch : 2 - step : 7/100 - loss: 3.093414545059204\n",
      "epoch : 2 - step : 8/100 - loss: 2.6797683238983154\n",
      "epoch : 2 - step : 9/100 - loss: 5.114637851715088\n",
      "epoch : 2 - step : 10/100 - loss: 3.3077759742736816\n",
      "epoch : 2 - step : 11/100 - loss: 2.8443753719329834\n",
      "epoch : 2 - step : 12/100 - loss: 3.3264288902282715\n",
      "epoch : 2 - step : 13/100 - loss: 3.886211395263672\n",
      "epoch : 2 - step : 14/100 - loss: 3.705390691757202\n",
      "epoch : 2 - step : 15/100 - loss: 4.219462871551514\n",
      "epoch : 2 - step : 16/100 - loss: 3.8457140922546387\n",
      "epoch : 2 - step : 17/100 - loss: 3.1270666122436523\n",
      "epoch : 2 - step : 18/100 - loss: 3.3115601539611816\n",
      "epoch : 2 - step : 19/100 - loss: 3.4643678665161133\n",
      "epoch : 2 - step : 20/100 - loss: 2.9981491565704346\n",
      "epoch : 2 - step : 21/100 - loss: 3.3040223121643066\n",
      "epoch : 2 - step : 22/100 - loss: 4.264615058898926\n",
      "epoch : 2 - step : 23/100 - loss: 4.737342834472656\n",
      "epoch : 2 - step : 24/100 - loss: 2.6560800075531006\n",
      "epoch : 2 - step : 25/100 - loss: 3.493405342102051\n",
      "epoch : 2 - step : 26/100 - loss: 3.170621871948242\n",
      "epoch : 2 - step : 27/100 - loss: 3.719198226928711\n",
      "epoch : 2 - step : 28/100 - loss: 3.4778997898101807\n",
      "epoch : 2 - step : 29/100 - loss: 3.8761141300201416\n",
      "epoch : 2 - step : 30/100 - loss: 3.511403799057007\n",
      "epoch : 2 - step : 31/100 - loss: 3.7288317680358887\n",
      "epoch : 2 - step : 32/100 - loss: 2.6107561588287354\n",
      "epoch : 2 - step : 33/100 - loss: 3.716615915298462\n",
      "epoch : 2 - step : 34/100 - loss: 4.454389572143555\n",
      "epoch : 2 - step : 35/100 - loss: 3.3652117252349854\n",
      "epoch : 2 - step : 36/100 - loss: 3.278968334197998\n",
      "epoch : 2 - step : 37/100 - loss: 3.7641496658325195\n",
      "epoch : 2 - step : 38/100 - loss: 4.623298645019531\n",
      "epoch : 2 - step : 39/100 - loss: 2.8431408405303955\n",
      "epoch : 2 - step : 40/100 - loss: 3.17897891998291\n",
      "epoch : 2 - step : 41/100 - loss: 3.12178897857666\n",
      "epoch : 2 - step : 42/100 - loss: 3.1548542976379395\n",
      "epoch : 2 - step : 43/100 - loss: 3.225800037384033\n",
      "epoch : 2 - step : 44/100 - loss: 3.8606982231140137\n",
      "epoch : 2 - step : 45/100 - loss: 2.9306228160858154\n",
      "epoch : 2 - step : 46/100 - loss: 2.873070001602173\n",
      "epoch : 2 - step : 47/100 - loss: 3.545795440673828\n",
      "epoch : 2 - step : 48/100 - loss: 2.4883553981781006\n",
      "epoch : 2 - step : 49/100 - loss: 2.4458346366882324\n",
      "epoch : 2 - step : 50/100 - loss: 3.4325568675994873\n",
      "epoch : 2 - step : 51/100 - loss: 2.577368974685669\n",
      "epoch : 2 - step : 52/100 - loss: 2.971865177154541\n",
      "epoch : 2 - step : 53/100 - loss: 2.7933382987976074\n",
      "epoch : 2 - step : 54/100 - loss: 3.2184741497039795\n",
      "epoch : 2 - step : 55/100 - loss: 2.7152538299560547\n",
      "epoch : 2 - step : 56/100 - loss: 2.9397733211517334\n",
      "epoch : 2 - step : 57/100 - loss: 2.9052810668945312\n",
      "epoch : 2 - step : 58/100 - loss: 4.125956058502197\n",
      "epoch : 2 - step : 59/100 - loss: 3.00002121925354\n",
      "epoch : 2 - step : 60/100 - loss: 2.796765089035034\n",
      "epoch : 2 - step : 61/100 - loss: 3.465184211730957\n",
      "epoch : 2 - step : 62/100 - loss: 2.4611711502075195\n",
      "epoch : 2 - step : 63/100 - loss: 2.139378547668457\n",
      "epoch : 2 - step : 64/100 - loss: 3.050903081893921\n",
      "epoch : 2 - step : 65/100 - loss: 3.341022491455078\n",
      "epoch : 2 - step : 66/100 - loss: 2.494494915008545\n",
      "epoch : 2 - step : 67/100 - loss: 3.237624168395996\n",
      "epoch : 2 - step : 68/100 - loss: 2.9995405673980713\n",
      "epoch : 2 - step : 69/100 - loss: 3.3937878608703613\n",
      "epoch : 2 - step : 70/100 - loss: 2.9706530570983887\n",
      "epoch : 2 - step : 71/100 - loss: 3.440380334854126\n",
      "epoch : 2 - step : 72/100 - loss: 3.048884630203247\n",
      "epoch : 2 - step : 73/100 - loss: 2.843456983566284\n",
      "epoch : 2 - step : 74/100 - loss: 2.910048007965088\n",
      "epoch : 2 - step : 75/100 - loss: 3.4122440814971924\n",
      "epoch : 2 - step : 76/100 - loss: 3.2942585945129395\n",
      "epoch : 2 - step : 77/100 - loss: 2.644944429397583\n",
      "epoch : 2 - step : 78/100 - loss: 2.624084949493408\n",
      "epoch : 2 - step : 79/100 - loss: 3.053645372390747\n",
      "epoch : 2 - step : 80/100 - loss: 2.9170117378234863\n",
      "epoch : 2 - step : 81/100 - loss: 2.8679609298706055\n",
      "epoch : 2 - step : 82/100 - loss: 3.005464792251587\n",
      "epoch : 2 - step : 83/100 - loss: 2.1469907760620117\n",
      "epoch : 2 - step : 84/100 - loss: 3.3119282722473145\n",
      "epoch : 2 - step : 85/100 - loss: 2.6083600521087646\n",
      "epoch : 2 - step : 86/100 - loss: 2.7600157260894775\n",
      "epoch : 2 - step : 87/100 - loss: 3.5389275550842285\n",
      "epoch : 2 - step : 88/100 - loss: 2.6452033519744873\n",
      "epoch : 2 - step : 89/100 - loss: 2.7483608722686768\n",
      "epoch : 2 - step : 90/100 - loss: 2.5259532928466797\n",
      "epoch : 2 - step : 91/100 - loss: 2.9488587379455566\n",
      "epoch : 2 - step : 92/100 - loss: 2.446934938430786\n",
      "epoch : 2 - step : 93/100 - loss: 2.1840875148773193\n",
      "epoch : 2 - step : 94/100 - loss: 2.809535503387451\n",
      "epoch : 2 - step : 95/100 - loss: 2.861694097518921\n",
      "epoch : 2 - step : 96/100 - loss: 2.723421812057495\n",
      "epoch : 2 - step : 97/100 - loss: 2.68379545211792\n",
      "epoch : 2 - step : 98/100 - loss: 2.8508591651916504\n",
      "epoch : 2 - step : 99/100 - loss: 2.5286967754364014\n",
      "Epoch: 2, train_Loss:  3.2059955644607543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [12:25<49:43, 372.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Valid_Loss:  2.4576582527160644\n",
      "epoch : 3 - step : 0/100 - loss: 2.565932273864746\n",
      "epoch : 3 - step : 1/100 - loss: 2.252896785736084\n",
      "epoch : 3 - step : 2/100 - loss: 2.4722654819488525\n",
      "epoch : 3 - step : 3/100 - loss: 2.4698617458343506\n",
      "epoch : 3 - step : 4/100 - loss: 2.4050004482269287\n",
      "epoch : 3 - step : 5/100 - loss: 2.3630874156951904\n",
      "epoch : 3 - step : 6/100 - loss: 2.2463390827178955\n",
      "epoch : 3 - step : 7/100 - loss: 2.424567461013794\n",
      "epoch : 3 - step : 8/100 - loss: 2.161353826522827\n",
      "epoch : 3 - step : 9/100 - loss: 2.0373098850250244\n",
      "epoch : 3 - step : 10/100 - loss: 2.352088451385498\n",
      "epoch : 3 - step : 11/100 - loss: 2.0062639713287354\n",
      "epoch : 3 - step : 12/100 - loss: 2.029745578765869\n",
      "epoch : 3 - step : 13/100 - loss: 2.09151029586792\n",
      "epoch : 3 - step : 14/100 - loss: 2.285493850708008\n",
      "epoch : 3 - step : 15/100 - loss: 2.0995571613311768\n",
      "epoch : 3 - step : 16/100 - loss: 2.195481300354004\n",
      "epoch : 3 - step : 17/100 - loss: 1.9171351194381714\n",
      "epoch : 3 - step : 18/100 - loss: 1.9924063682556152\n",
      "epoch : 3 - step : 19/100 - loss: 1.8089689016342163\n",
      "epoch : 3 - step : 20/100 - loss: 1.8080413341522217\n",
      "epoch : 3 - step : 21/100 - loss: 1.6635563373565674\n",
      "epoch : 3 - step : 22/100 - loss: 1.6030551195144653\n",
      "epoch : 3 - step : 23/100 - loss: 1.5574562549591064\n",
      "epoch : 3 - step : 24/100 - loss: 1.5562509298324585\n",
      "epoch : 3 - step : 25/100 - loss: 1.5063334703445435\n",
      "epoch : 3 - step : 26/100 - loss: 1.4176366329193115\n",
      "epoch : 3 - step : 27/100 - loss: 1.3939791917800903\n",
      "epoch : 3 - step : 28/100 - loss: 1.538214087486267\n",
      "epoch : 3 - step : 29/100 - loss: 1.529929518699646\n",
      "epoch : 3 - step : 30/100 - loss: 1.3344000577926636\n",
      "epoch : 3 - step : 31/100 - loss: 1.4238120317459106\n",
      "epoch : 3 - step : 32/100 - loss: 1.2241624593734741\n",
      "epoch : 3 - step : 33/100 - loss: 1.24578058719635\n",
      "epoch : 3 - step : 34/100 - loss: 1.025772213935852\n",
      "epoch : 3 - step : 35/100 - loss: 1.1141289472579956\n",
      "epoch : 3 - step : 36/100 - loss: 1.4433997869491577\n",
      "epoch : 3 - step : 37/100 - loss: 1.3351316452026367\n",
      "epoch : 3 - step : 38/100 - loss: 1.1791610717773438\n",
      "epoch : 3 - step : 39/100 - loss: 1.2031970024108887\n",
      "epoch : 3 - step : 40/100 - loss: 1.32970130443573\n",
      "epoch : 3 - step : 41/100 - loss: 1.2460076808929443\n",
      "epoch : 3 - step : 42/100 - loss: 1.304003119468689\n",
      "epoch : 3 - step : 43/100 - loss: 1.1905272006988525\n",
      "epoch : 3 - step : 44/100 - loss: 1.3382028341293335\n",
      "epoch : 3 - step : 45/100 - loss: 1.156347393989563\n",
      "epoch : 3 - step : 46/100 - loss: 1.1350444555282593\n",
      "epoch : 3 - step : 47/100 - loss: 1.2943894863128662\n",
      "epoch : 3 - step : 48/100 - loss: 1.1731818914413452\n",
      "epoch : 3 - step : 49/100 - loss: 1.040080189704895\n",
      "epoch : 3 - step : 50/100 - loss: 1.3380218744277954\n",
      "epoch : 3 - step : 51/100 - loss: 1.2634562253952026\n",
      "epoch : 3 - step : 52/100 - loss: 1.3040611743927002\n",
      "epoch : 3 - step : 53/100 - loss: 1.1911460161209106\n",
      "epoch : 3 - step : 54/100 - loss: 1.0769039392471313\n",
      "epoch : 3 - step : 55/100 - loss: 1.4129124879837036\n",
      "epoch : 3 - step : 56/100 - loss: 1.0488604307174683\n",
      "epoch : 3 - step : 57/100 - loss: 1.1897122859954834\n",
      "epoch : 3 - step : 58/100 - loss: 0.8528222441673279\n",
      "epoch : 3 - step : 59/100 - loss: 1.1290377378463745\n",
      "epoch : 3 - step : 60/100 - loss: 1.3067970275878906\n",
      "epoch : 3 - step : 61/100 - loss: 1.2607781887054443\n",
      "epoch : 3 - step : 62/100 - loss: 0.9544917941093445\n",
      "epoch : 3 - step : 63/100 - loss: 1.1690819263458252\n",
      "epoch : 3 - step : 64/100 - loss: 1.1474710702896118\n",
      "epoch : 3 - step : 65/100 - loss: 1.2260689735412598\n",
      "epoch : 3 - step : 66/100 - loss: 1.0371900796890259\n",
      "epoch : 3 - step : 67/100 - loss: 1.2888635396957397\n",
      "epoch : 3 - step : 68/100 - loss: 0.9863168597221375\n",
      "epoch : 3 - step : 69/100 - loss: 0.679851233959198\n",
      "epoch : 3 - step : 70/100 - loss: 1.2119337320327759\n",
      "epoch : 3 - step : 71/100 - loss: 1.28852379322052\n",
      "epoch : 3 - step : 72/100 - loss: 1.2793713808059692\n",
      "epoch : 3 - step : 73/100 - loss: 1.2483800649642944\n",
      "epoch : 3 - step : 74/100 - loss: 1.0925132036209106\n",
      "epoch : 3 - step : 75/100 - loss: 0.9106945395469666\n",
      "epoch : 3 - step : 76/100 - loss: 0.9321844577789307\n",
      "epoch : 3 - step : 77/100 - loss: 1.3920679092407227\n",
      "epoch : 3 - step : 78/100 - loss: 1.40912926197052\n",
      "epoch : 3 - step : 79/100 - loss: 1.3427284955978394\n",
      "epoch : 3 - step : 80/100 - loss: 1.3421934843063354\n",
      "epoch : 3 - step : 81/100 - loss: 1.111274003982544\n",
      "epoch : 3 - step : 82/100 - loss: 1.0143499374389648\n",
      "epoch : 3 - step : 83/100 - loss: 1.070603609085083\n",
      "epoch : 3 - step : 84/100 - loss: 1.1663769483566284\n",
      "epoch : 3 - step : 85/100 - loss: 1.1669317483901978\n",
      "epoch : 3 - step : 86/100 - loss: 1.116689920425415\n",
      "epoch : 3 - step : 87/100 - loss: 1.5119669437408447\n",
      "epoch : 3 - step : 88/100 - loss: 1.04524827003479\n",
      "epoch : 3 - step : 89/100 - loss: 1.048622965812683\n",
      "epoch : 3 - step : 90/100 - loss: 1.2656868696212769\n",
      "epoch : 3 - step : 91/100 - loss: 1.284340262413025\n",
      "epoch : 3 - step : 92/100 - loss: 1.1253137588500977\n",
      "epoch : 3 - step : 93/100 - loss: 1.0605626106262207\n",
      "epoch : 3 - step : 94/100 - loss: 1.4800047874450684\n",
      "epoch : 3 - step : 95/100 - loss: 1.0538911819458008\n",
      "epoch : 3 - step : 96/100 - loss: 1.2900134325027466\n",
      "epoch : 3 - step : 97/100 - loss: 0.9672755599021912\n",
      "epoch : 3 - step : 98/100 - loss: 1.0991758108139038\n",
      "epoch : 3 - step : 99/100 - loss: 1.2410740852355957\n",
      "Epoch: 3, train_Loss:  1.4292111778259278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [18:40<43:38, 374.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Valid_Loss:  1.18987957239151\n",
      "epoch : 4 - step : 0/100 - loss: 1.0777610540390015\n",
      "epoch : 4 - step : 1/100 - loss: 1.1038901805877686\n",
      "epoch : 4 - step : 2/100 - loss: 0.9783695340156555\n",
      "epoch : 4 - step : 3/100 - loss: 1.0689506530761719\n",
      "epoch : 4 - step : 4/100 - loss: 1.2480379343032837\n",
      "epoch : 4 - step : 5/100 - loss: 1.3468263149261475\n",
      "epoch : 4 - step : 6/100 - loss: 1.2001373767852783\n",
      "epoch : 4 - step : 7/100 - loss: 1.0687154531478882\n",
      "epoch : 4 - step : 8/100 - loss: 1.1311532258987427\n",
      "epoch : 4 - step : 9/100 - loss: 1.1260300874710083\n",
      "epoch : 4 - step : 10/100 - loss: 1.026847243309021\n",
      "epoch : 4 - step : 11/100 - loss: 1.1798378229141235\n",
      "epoch : 4 - step : 12/100 - loss: 1.2726625204086304\n",
      "epoch : 4 - step : 13/100 - loss: 1.0348143577575684\n",
      "epoch : 4 - step : 14/100 - loss: 1.1563081741333008\n",
      "epoch : 4 - step : 15/100 - loss: 1.3850759267807007\n",
      "epoch : 4 - step : 16/100 - loss: 1.2643425464630127\n",
      "epoch : 4 - step : 17/100 - loss: 1.1615980863571167\n",
      "epoch : 4 - step : 18/100 - loss: 1.349873661994934\n",
      "epoch : 4 - step : 19/100 - loss: 1.1811857223510742\n",
      "epoch : 4 - step : 20/100 - loss: 0.9880023002624512\n",
      "epoch : 4 - step : 21/100 - loss: 0.8875916600227356\n",
      "epoch : 4 - step : 22/100 - loss: 1.1400952339172363\n",
      "epoch : 4 - step : 23/100 - loss: 1.1990032196044922\n",
      "epoch : 4 - step : 24/100 - loss: 1.045811414718628\n",
      "epoch : 4 - step : 25/100 - loss: 1.0257208347320557\n",
      "epoch : 4 - step : 26/100 - loss: 1.3923556804656982\n",
      "epoch : 4 - step : 27/100 - loss: 1.4433056116104126\n",
      "epoch : 4 - step : 28/100 - loss: 1.1547424793243408\n",
      "epoch : 4 - step : 29/100 - loss: 1.3775954246520996\n",
      "epoch : 4 - step : 30/100 - loss: 1.1228517293930054\n",
      "epoch : 4 - step : 31/100 - loss: 1.1180920600891113\n",
      "epoch : 4 - step : 32/100 - loss: 1.3089967966079712\n",
      "epoch : 4 - step : 33/100 - loss: 1.1600788831710815\n",
      "epoch : 4 - step : 34/100 - loss: 1.255990982055664\n",
      "epoch : 4 - step : 35/100 - loss: 1.2459965944290161\n",
      "epoch : 4 - step : 36/100 - loss: 1.0060005187988281\n",
      "epoch : 4 - step : 37/100 - loss: 1.1386219263076782\n",
      "epoch : 4 - step : 38/100 - loss: 1.2731069326400757\n",
      "epoch : 4 - step : 39/100 - loss: 1.0285810232162476\n",
      "epoch : 4 - step : 40/100 - loss: 1.2998062372207642\n",
      "epoch : 4 - step : 41/100 - loss: 1.0521005392074585\n",
      "epoch : 4 - step : 42/100 - loss: 1.346807837486267\n",
      "epoch : 4 - step : 43/100 - loss: 1.2643996477127075\n",
      "epoch : 4 - step : 44/100 - loss: 1.3646461963653564\n",
      "epoch : 4 - step : 45/100 - loss: 1.0198832750320435\n",
      "epoch : 4 - step : 46/100 - loss: 1.1977839469909668\n",
      "epoch : 4 - step : 47/100 - loss: 1.1065442562103271\n",
      "epoch : 4 - step : 48/100 - loss: 1.1339656114578247\n",
      "epoch : 4 - step : 49/100 - loss: 1.0215089321136475\n",
      "epoch : 4 - step : 50/100 - loss: 1.240641474723816\n",
      "epoch : 4 - step : 51/100 - loss: 1.0604753494262695\n",
      "epoch : 4 - step : 52/100 - loss: 1.1860688924789429\n",
      "epoch : 4 - step : 53/100 - loss: 0.7497408986091614\n",
      "epoch : 4 - step : 54/100 - loss: 1.3264826536178589\n",
      "epoch : 4 - step : 55/100 - loss: 1.1622074842453003\n",
      "epoch : 4 - step : 56/100 - loss: 1.045108437538147\n",
      "epoch : 4 - step : 57/100 - loss: 1.1615111827850342\n",
      "epoch : 4 - step : 58/100 - loss: 1.196068525314331\n",
      "epoch : 4 - step : 59/100 - loss: 1.335325002670288\n",
      "epoch : 4 - step : 60/100 - loss: 1.1352860927581787\n",
      "epoch : 4 - step : 61/100 - loss: 1.3581016063690186\n",
      "epoch : 4 - step : 62/100 - loss: 1.0496433973312378\n",
      "epoch : 4 - step : 63/100 - loss: 1.0426225662231445\n",
      "epoch : 4 - step : 64/100 - loss: 0.8655376434326172\n",
      "epoch : 4 - step : 65/100 - loss: 1.221388816833496\n",
      "epoch : 4 - step : 66/100 - loss: 1.0585143566131592\n",
      "epoch : 4 - step : 67/100 - loss: 1.069628119468689\n",
      "epoch : 4 - step : 68/100 - loss: 1.2503033876419067\n",
      "epoch : 4 - step : 69/100 - loss: 1.0885939598083496\n",
      "epoch : 4 - step : 70/100 - loss: 1.2664737701416016\n",
      "epoch : 4 - step : 71/100 - loss: 1.024945616722107\n",
      "epoch : 4 - step : 72/100 - loss: 1.0795402526855469\n",
      "epoch : 4 - step : 73/100 - loss: 1.1758606433868408\n",
      "epoch : 4 - step : 74/100 - loss: 0.9912842512130737\n",
      "epoch : 4 - step : 75/100 - loss: 0.8043357729911804\n",
      "epoch : 4 - step : 76/100 - loss: 0.7799666523933411\n",
      "epoch : 4 - step : 77/100 - loss: 0.6633736491203308\n",
      "epoch : 4 - step : 78/100 - loss: 1.107383370399475\n",
      "epoch : 4 - step : 79/100 - loss: 1.1678767204284668\n",
      "epoch : 4 - step : 80/100 - loss: 1.0151548385620117\n",
      "epoch : 4 - step : 81/100 - loss: 0.9557124376296997\n",
      "epoch : 4 - step : 82/100 - loss: 1.16532301902771\n",
      "epoch : 4 - step : 83/100 - loss: 1.175222635269165\n",
      "epoch : 4 - step : 84/100 - loss: 1.0789661407470703\n",
      "epoch : 4 - step : 85/100 - loss: 1.0775116682052612\n",
      "epoch : 4 - step : 86/100 - loss: 0.9465165734291077\n",
      "epoch : 4 - step : 87/100 - loss: 0.8278734087944031\n",
      "epoch : 4 - step : 88/100 - loss: 0.9374781847000122\n",
      "epoch : 4 - step : 89/100 - loss: 1.3150476217269897\n",
      "epoch : 4 - step : 90/100 - loss: 1.0682252645492554\n",
      "epoch : 4 - step : 91/100 - loss: 0.9875943660736084\n",
      "epoch : 4 - step : 92/100 - loss: 1.1449519395828247\n",
      "epoch : 4 - step : 93/100 - loss: 1.206660509109497\n",
      "epoch : 4 - step : 94/100 - loss: 1.274686574935913\n",
      "epoch : 4 - step : 95/100 - loss: 1.0602318048477173\n",
      "epoch : 4 - step : 96/100 - loss: 1.080617070198059\n",
      "epoch : 4 - step : 97/100 - loss: 0.7273545265197754\n",
      "epoch : 4 - step : 98/100 - loss: 1.0715746879577637\n",
      "epoch : 4 - step : 99/100 - loss: 0.9799822568893433\n",
      "Epoch: 4, train_Loss:  1.1224338173866273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 4/10 [24:55<37:24, 374.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Valid_Loss:  1.1563247561454773\n",
      "epoch : 5 - step : 0/100 - loss: 0.9507243633270264\n",
      "epoch : 5 - step : 1/100 - loss: 1.459241271018982\n",
      "epoch : 5 - step : 2/100 - loss: 1.3503077030181885\n",
      "epoch : 5 - step : 3/100 - loss: 0.9100711941719055\n",
      "epoch : 5 - step : 4/100 - loss: 1.2550173997879028\n",
      "epoch : 5 - step : 5/100 - loss: 1.1668814420700073\n",
      "epoch : 5 - step : 6/100 - loss: 1.2165910005569458\n",
      "epoch : 5 - step : 7/100 - loss: 1.134772539138794\n",
      "epoch : 5 - step : 8/100 - loss: 1.1548123359680176\n",
      "epoch : 5 - step : 9/100 - loss: 1.185459017753601\n",
      "epoch : 5 - step : 10/100 - loss: 1.1787217855453491\n",
      "epoch : 5 - step : 11/100 - loss: 1.1520094871520996\n",
      "epoch : 5 - step : 12/100 - loss: 0.8981131911277771\n",
      "epoch : 5 - step : 13/100 - loss: 0.8694112300872803\n",
      "epoch : 5 - step : 14/100 - loss: 1.0787346363067627\n",
      "epoch : 5 - step : 15/100 - loss: 1.2829562425613403\n",
      "epoch : 5 - step : 16/100 - loss: 1.3118489980697632\n",
      "epoch : 5 - step : 17/100 - loss: 1.180798888206482\n",
      "epoch : 5 - step : 18/100 - loss: 1.2312818765640259\n",
      "epoch : 5 - step : 19/100 - loss: 1.0119105577468872\n",
      "epoch : 5 - step : 20/100 - loss: 0.7411234974861145\n",
      "epoch : 5 - step : 21/100 - loss: 1.008410096168518\n",
      "epoch : 5 - step : 22/100 - loss: 1.0340591669082642\n",
      "epoch : 5 - step : 23/100 - loss: 1.0872725248336792\n",
      "epoch : 5 - step : 24/100 - loss: 1.1012966632843018\n",
      "epoch : 5 - step : 25/100 - loss: 1.1360348463058472\n",
      "epoch : 5 - step : 26/100 - loss: 0.9775189161300659\n",
      "epoch : 5 - step : 27/100 - loss: 1.3163504600524902\n",
      "epoch : 5 - step : 28/100 - loss: 1.1351600885391235\n",
      "epoch : 5 - step : 29/100 - loss: 1.076767921447754\n",
      "epoch : 5 - step : 30/100 - loss: 1.254838228225708\n",
      "epoch : 5 - step : 31/100 - loss: 1.0776978731155396\n",
      "epoch : 5 - step : 32/100 - loss: 1.091691017150879\n",
      "epoch : 5 - step : 33/100 - loss: 1.1570446491241455\n",
      "epoch : 5 - step : 34/100 - loss: 1.1425557136535645\n",
      "epoch : 5 - step : 35/100 - loss: 0.9482395648956299\n",
      "epoch : 5 - step : 36/100 - loss: 0.8967137336730957\n",
      "epoch : 5 - step : 37/100 - loss: 1.1239937543869019\n",
      "epoch : 5 - step : 38/100 - loss: 1.3367060422897339\n",
      "epoch : 5 - step : 39/100 - loss: 0.9847462773323059\n",
      "epoch : 5 - step : 40/100 - loss: 1.2267265319824219\n",
      "epoch : 5 - step : 41/100 - loss: 1.196832299232483\n",
      "epoch : 5 - step : 42/100 - loss: 1.0323841571807861\n",
      "epoch : 5 - step : 43/100 - loss: 1.245706558227539\n",
      "epoch : 5 - step : 44/100 - loss: 1.0358153581619263\n",
      "epoch : 5 - step : 45/100 - loss: 1.0565857887268066\n",
      "epoch : 5 - step : 46/100 - loss: 1.2831815481185913\n",
      "epoch : 5 - step : 47/100 - loss: 0.9583791494369507\n",
      "epoch : 5 - step : 48/100 - loss: 0.6761848330497742\n",
      "epoch : 5 - step : 49/100 - loss: 1.2218025922775269\n",
      "epoch : 5 - step : 50/100 - loss: 1.388970971107483\n",
      "epoch : 5 - step : 51/100 - loss: 1.0061039924621582\n",
      "epoch : 5 - step : 52/100 - loss: 1.1427932977676392\n",
      "epoch : 5 - step : 53/100 - loss: 0.9535228610038757\n",
      "epoch : 5 - step : 54/100 - loss: 0.9977628588676453\n",
      "epoch : 5 - step : 55/100 - loss: 1.0863851308822632\n",
      "epoch : 5 - step : 56/100 - loss: 1.0058326721191406\n",
      "epoch : 5 - step : 57/100 - loss: 1.1331545114517212\n",
      "epoch : 5 - step : 58/100 - loss: 1.1073006391525269\n",
      "epoch : 5 - step : 59/100 - loss: 1.3274204730987549\n",
      "epoch : 5 - step : 60/100 - loss: 1.1153736114501953\n",
      "epoch : 5 - step : 61/100 - loss: 0.9716016054153442\n",
      "epoch : 5 - step : 62/100 - loss: 1.1267809867858887\n",
      "epoch : 5 - step : 63/100 - loss: 0.9612607359886169\n",
      "epoch : 5 - step : 64/100 - loss: 1.0302939414978027\n",
      "epoch : 5 - step : 65/100 - loss: 1.3137454986572266\n",
      "epoch : 5 - step : 66/100 - loss: 1.1178219318389893\n",
      "epoch : 5 - step : 67/100 - loss: 1.026557207107544\n",
      "epoch : 5 - step : 68/100 - loss: 1.032880187034607\n",
      "epoch : 5 - step : 69/100 - loss: 1.0397770404815674\n",
      "epoch : 5 - step : 70/100 - loss: 1.123372197151184\n",
      "epoch : 5 - step : 71/100 - loss: 1.0407449007034302\n",
      "epoch : 5 - step : 72/100 - loss: 1.0404424667358398\n",
      "epoch : 5 - step : 73/100 - loss: 0.9836022853851318\n",
      "epoch : 5 - step : 74/100 - loss: 1.2077933549880981\n",
      "epoch : 5 - step : 75/100 - loss: 1.105142593383789\n",
      "epoch : 5 - step : 76/100 - loss: 1.0157748460769653\n",
      "epoch : 5 - step : 77/100 - loss: 1.2310526371002197\n",
      "epoch : 5 - step : 78/100 - loss: 1.240805745124817\n",
      "epoch : 5 - step : 79/100 - loss: 1.2003823518753052\n",
      "epoch : 5 - step : 80/100 - loss: 1.2188915014266968\n",
      "epoch : 5 - step : 81/100 - loss: 1.1361339092254639\n",
      "epoch : 5 - step : 82/100 - loss: 1.169780969619751\n",
      "epoch : 5 - step : 83/100 - loss: 0.7668682336807251\n",
      "epoch : 5 - step : 84/100 - loss: 0.7018952965736389\n",
      "epoch : 5 - step : 85/100 - loss: 1.2000045776367188\n",
      "epoch : 5 - step : 86/100 - loss: 0.9997813701629639\n",
      "epoch : 5 - step : 87/100 - loss: 1.0387237071990967\n",
      "epoch : 5 - step : 88/100 - loss: 0.8444198966026306\n",
      "epoch : 5 - step : 89/100 - loss: 0.9574881792068481\n",
      "epoch : 5 - step : 90/100 - loss: 1.0762916803359985\n",
      "epoch : 5 - step : 91/100 - loss: 1.3263615369796753\n",
      "epoch : 5 - step : 92/100 - loss: 1.0248725414276123\n",
      "epoch : 5 - step : 93/100 - loss: 0.9043506979942322\n",
      "epoch : 5 - step : 94/100 - loss: 1.2886862754821777\n",
      "epoch : 5 - step : 95/100 - loss: 0.7612352967262268\n",
      "epoch : 5 - step : 96/100 - loss: 1.1030449867248535\n",
      "epoch : 5 - step : 97/100 - loss: 1.1044477224349976\n",
      "epoch : 5 - step : 98/100 - loss: 1.1108572483062744\n",
      "epoch : 5 - step : 99/100 - loss: 0.7959879040718079\n",
      "Epoch: 5, train_Loss:  1.0914606207609177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 5/10 [31:10<31:12, 374.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Valid_Loss:  1.1333156967163085\n",
      "epoch : 6 - step : 0/100 - loss: 1.2100551128387451\n",
      "epoch : 6 - step : 1/100 - loss: 1.2006744146347046\n",
      "epoch : 6 - step : 2/100 - loss: 1.0828043222427368\n",
      "epoch : 6 - step : 3/100 - loss: 0.8785880208015442\n",
      "epoch : 6 - step : 4/100 - loss: 0.9538809657096863\n",
      "epoch : 6 - step : 5/100 - loss: 1.0412789583206177\n",
      "epoch : 6 - step : 6/100 - loss: 1.0823692083358765\n",
      "epoch : 6 - step : 7/100 - loss: 0.9864757657051086\n",
      "epoch : 6 - step : 8/100 - loss: 0.9890445470809937\n",
      "epoch : 6 - step : 9/100 - loss: 1.1520756483078003\n",
      "epoch : 6 - step : 10/100 - loss: 0.9985886812210083\n",
      "epoch : 6 - step : 11/100 - loss: 1.120221734046936\n",
      "epoch : 6 - step : 12/100 - loss: 1.1822255849838257\n",
      "epoch : 6 - step : 13/100 - loss: 0.9229302406311035\n",
      "epoch : 6 - step : 14/100 - loss: 1.1914290189743042\n",
      "epoch : 6 - step : 15/100 - loss: 1.2036080360412598\n",
      "epoch : 6 - step : 16/100 - loss: 1.1680525541305542\n",
      "epoch : 6 - step : 17/100 - loss: 0.7060254216194153\n",
      "epoch : 6 - step : 18/100 - loss: 0.7014274001121521\n",
      "epoch : 6 - step : 19/100 - loss: 1.1926203966140747\n",
      "epoch : 6 - step : 20/100 - loss: 0.9979650378227234\n",
      "epoch : 6 - step : 21/100 - loss: 1.0593793392181396\n",
      "epoch : 6 - step : 22/100 - loss: 1.0030736923217773\n",
      "epoch : 6 - step : 23/100 - loss: 1.1355273723602295\n",
      "epoch : 6 - step : 24/100 - loss: 0.8836365938186646\n",
      "epoch : 6 - step : 25/100 - loss: 1.163889765739441\n",
      "epoch : 6 - step : 26/100 - loss: 1.2631467580795288\n",
      "epoch : 6 - step : 27/100 - loss: 1.0497609376907349\n",
      "epoch : 6 - step : 28/100 - loss: 1.335218071937561\n",
      "epoch : 6 - step : 29/100 - loss: 1.2811195850372314\n",
      "epoch : 6 - step : 30/100 - loss: 1.1039857864379883\n",
      "epoch : 6 - step : 31/100 - loss: 0.8076201677322388\n",
      "epoch : 6 - step : 32/100 - loss: 0.9490353465080261\n",
      "epoch : 6 - step : 33/100 - loss: 1.0831875801086426\n",
      "epoch : 6 - step : 34/100 - loss: 1.2302396297454834\n",
      "epoch : 6 - step : 35/100 - loss: 1.2083269357681274\n",
      "epoch : 6 - step : 36/100 - loss: 0.9712661504745483\n",
      "epoch : 6 - step : 37/100 - loss: 1.216383457183838\n",
      "epoch : 6 - step : 38/100 - loss: 1.200875163078308\n",
      "epoch : 6 - step : 39/100 - loss: 1.1984153985977173\n",
      "epoch : 6 - step : 40/100 - loss: 1.2095900774002075\n",
      "epoch : 6 - step : 41/100 - loss: 1.0030901432037354\n",
      "epoch : 6 - step : 42/100 - loss: 1.0181198120117188\n",
      "epoch : 6 - step : 43/100 - loss: 1.0739917755126953\n",
      "epoch : 6 - step : 44/100 - loss: 0.9700220227241516\n",
      "epoch : 6 - step : 45/100 - loss: 1.1675500869750977\n",
      "epoch : 6 - step : 46/100 - loss: 1.1364387273788452\n",
      "epoch : 6 - step : 47/100 - loss: 0.9236201047897339\n",
      "epoch : 6 - step : 48/100 - loss: 1.0265856981277466\n",
      "epoch : 6 - step : 49/100 - loss: 1.151625394821167\n",
      "epoch : 6 - step : 50/100 - loss: 1.1995115280151367\n",
      "epoch : 6 - step : 51/100 - loss: 1.0500375032424927\n",
      "epoch : 6 - step : 52/100 - loss: 0.8001512885093689\n",
      "epoch : 6 - step : 53/100 - loss: 1.2383400201797485\n",
      "epoch : 6 - step : 54/100 - loss: 1.0221532583236694\n",
      "epoch : 6 - step : 55/100 - loss: 1.0486515760421753\n",
      "epoch : 6 - step : 56/100 - loss: 1.0286461114883423\n",
      "epoch : 6 - step : 57/100 - loss: 1.1779943704605103\n",
      "epoch : 6 - step : 58/100 - loss: 0.9426410794258118\n",
      "epoch : 6 - step : 59/100 - loss: 0.9977436065673828\n",
      "epoch : 6 - step : 60/100 - loss: 0.6699860692024231\n",
      "epoch : 6 - step : 61/100 - loss: 1.325384259223938\n",
      "epoch : 6 - step : 62/100 - loss: 1.0580781698226929\n",
      "epoch : 6 - step : 63/100 - loss: 0.8396543860435486\n",
      "epoch : 6 - step : 64/100 - loss: 0.8875206112861633\n",
      "epoch : 6 - step : 65/100 - loss: 1.0383994579315186\n",
      "epoch : 6 - step : 66/100 - loss: 1.3301258087158203\n",
      "epoch : 6 - step : 67/100 - loss: 1.0351473093032837\n",
      "epoch : 6 - step : 68/100 - loss: 1.075399398803711\n",
      "epoch : 6 - step : 69/100 - loss: 1.2942860126495361\n",
      "epoch : 6 - step : 70/100 - loss: 0.9178111553192139\n",
      "epoch : 6 - step : 71/100 - loss: 0.9432422518730164\n",
      "epoch : 6 - step : 72/100 - loss: 1.1337814331054688\n",
      "epoch : 6 - step : 73/100 - loss: 1.2687946557998657\n",
      "epoch : 6 - step : 74/100 - loss: 1.1882153749465942\n",
      "epoch : 6 - step : 75/100 - loss: 1.0993224382400513\n",
      "epoch : 6 - step : 76/100 - loss: 1.0421245098114014\n",
      "epoch : 6 - step : 77/100 - loss: 1.2216521501541138\n",
      "epoch : 6 - step : 78/100 - loss: 1.033832311630249\n",
      "epoch : 6 - step : 79/100 - loss: 1.211117148399353\n",
      "epoch : 6 - step : 80/100 - loss: 0.9950713515281677\n",
      "epoch : 6 - step : 81/100 - loss: 1.0080593824386597\n",
      "epoch : 6 - step : 82/100 - loss: 1.2075687646865845\n",
      "epoch : 6 - step : 83/100 - loss: 1.0793819427490234\n",
      "epoch : 6 - step : 84/100 - loss: 1.1548277139663696\n",
      "epoch : 6 - step : 85/100 - loss: 1.0480228662490845\n",
      "epoch : 6 - step : 86/100 - loss: 1.2045412063598633\n",
      "epoch : 6 - step : 87/100 - loss: 1.113869309425354\n",
      "epoch : 6 - step : 88/100 - loss: 0.9687519073486328\n",
      "epoch : 6 - step : 89/100 - loss: 1.1111531257629395\n",
      "epoch : 6 - step : 90/100 - loss: 1.1048035621643066\n",
      "epoch : 6 - step : 91/100 - loss: 1.3209903240203857\n",
      "epoch : 6 - step : 92/100 - loss: 0.948864758014679\n",
      "epoch : 6 - step : 93/100 - loss: 0.7680826783180237\n",
      "epoch : 6 - step : 94/100 - loss: 0.963844358921051\n",
      "epoch : 6 - step : 95/100 - loss: 0.9809290766716003\n",
      "epoch : 6 - step : 96/100 - loss: 1.1768146753311157\n",
      "epoch : 6 - step : 97/100 - loss: 1.0489221811294556\n",
      "epoch : 6 - step : 98/100 - loss: 1.0444221496582031\n",
      "epoch : 6 - step : 99/100 - loss: 0.7992833256721497\n",
      "Epoch: 6, train_Loss:  1.0695501255989075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 6/10 [37:26<25:00, 375.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Valid_Loss:  1.1111786103248595\n",
      "epoch : 7 - step : 0/100 - loss: 1.0150014162063599\n",
      "epoch : 7 - step : 1/100 - loss: 1.0982633829116821\n",
      "epoch : 7 - step : 2/100 - loss: 1.138224482536316\n",
      "epoch : 7 - step : 3/100 - loss: 1.2722336053848267\n",
      "epoch : 7 - step : 4/100 - loss: 0.6403920650482178\n",
      "epoch : 7 - step : 5/100 - loss: 0.9785756468772888\n",
      "epoch : 7 - step : 6/100 - loss: 1.2868642807006836\n",
      "epoch : 7 - step : 7/100 - loss: 0.7481834888458252\n",
      "epoch : 7 - step : 8/100 - loss: 1.073380470275879\n",
      "epoch : 7 - step : 9/100 - loss: 1.085951328277588\n",
      "epoch : 7 - step : 10/100 - loss: 1.259090542793274\n",
      "epoch : 7 - step : 11/100 - loss: 1.22862708568573\n",
      "epoch : 7 - step : 12/100 - loss: 1.0673229694366455\n",
      "epoch : 7 - step : 13/100 - loss: 1.0509519577026367\n",
      "epoch : 7 - step : 14/100 - loss: 1.06611967086792\n",
      "epoch : 7 - step : 15/100 - loss: 1.007073163986206\n",
      "epoch : 7 - step : 16/100 - loss: 1.1566424369812012\n",
      "epoch : 7 - step : 17/100 - loss: 1.0148100852966309\n",
      "epoch : 7 - step : 18/100 - loss: 1.121689796447754\n",
      "epoch : 7 - step : 19/100 - loss: 1.2100123167037964\n",
      "epoch : 7 - step : 20/100 - loss: 0.8471091389656067\n",
      "epoch : 7 - step : 21/100 - loss: 0.8922097086906433\n",
      "epoch : 7 - step : 22/100 - loss: 1.176790714263916\n",
      "epoch : 7 - step : 23/100 - loss: 0.7035272121429443\n",
      "epoch : 7 - step : 24/100 - loss: 1.1192588806152344\n",
      "epoch : 7 - step : 25/100 - loss: 1.2163333892822266\n",
      "epoch : 7 - step : 26/100 - loss: 0.9894117712974548\n",
      "epoch : 7 - step : 27/100 - loss: 1.2029986381530762\n",
      "epoch : 7 - step : 28/100 - loss: 1.0764678716659546\n",
      "epoch : 7 - step : 29/100 - loss: 1.2301515340805054\n",
      "epoch : 7 - step : 30/100 - loss: 1.180973768234253\n",
      "epoch : 7 - step : 31/100 - loss: 0.9814668893814087\n",
      "epoch : 7 - step : 32/100 - loss: 1.113447904586792\n",
      "epoch : 7 - step : 33/100 - loss: 0.8579603433609009\n",
      "epoch : 7 - step : 34/100 - loss: 0.9208837151527405\n",
      "epoch : 7 - step : 35/100 - loss: 1.0786646604537964\n",
      "epoch : 7 - step : 36/100 - loss: 1.1507819890975952\n",
      "epoch : 7 - step : 37/100 - loss: 1.2540310621261597\n",
      "epoch : 7 - step : 38/100 - loss: 1.0676581859588623\n",
      "epoch : 7 - step : 39/100 - loss: 0.968707799911499\n",
      "epoch : 7 - step : 40/100 - loss: 1.0995209217071533\n",
      "epoch : 7 - step : 41/100 - loss: 0.9706565141677856\n",
      "epoch : 7 - step : 42/100 - loss: 0.6620060205459595\n",
      "epoch : 7 - step : 43/100 - loss: 1.0358260869979858\n",
      "epoch : 7 - step : 44/100 - loss: 1.05502188205719\n",
      "epoch : 7 - step : 45/100 - loss: 1.048776388168335\n",
      "epoch : 7 - step : 46/100 - loss: 1.2617764472961426\n",
      "epoch : 7 - step : 47/100 - loss: 1.0492771863937378\n",
      "epoch : 7 - step : 48/100 - loss: 1.0786694288253784\n",
      "epoch : 7 - step : 49/100 - loss: 1.3331478834152222\n",
      "epoch : 7 - step : 50/100 - loss: 1.0472792387008667\n",
      "epoch : 7 - step : 51/100 - loss: 1.0969668626785278\n",
      "epoch : 7 - step : 52/100 - loss: 0.9489068984985352\n",
      "epoch : 7 - step : 53/100 - loss: 1.0784012079238892\n",
      "epoch : 7 - step : 54/100 - loss: 1.0437992811203003\n",
      "epoch : 7 - step : 55/100 - loss: 1.0499016046524048\n",
      "epoch : 7 - step : 56/100 - loss: 0.8763291835784912\n",
      "epoch : 7 - step : 57/100 - loss: 0.8475281596183777\n",
      "epoch : 7 - step : 58/100 - loss: 0.9165039658546448\n",
      "epoch : 7 - step : 59/100 - loss: 1.1296164989471436\n",
      "epoch : 7 - step : 60/100 - loss: 1.1336697340011597\n",
      "epoch : 7 - step : 61/100 - loss: 1.2508924007415771\n",
      "epoch : 7 - step : 62/100 - loss: 0.8256832957267761\n",
      "epoch : 7 - step : 63/100 - loss: 0.8864181041717529\n",
      "epoch : 7 - step : 64/100 - loss: 0.9751031398773193\n",
      "epoch : 7 - step : 65/100 - loss: 1.0794035196304321\n",
      "epoch : 7 - step : 66/100 - loss: 1.2552640438079834\n",
      "epoch : 7 - step : 67/100 - loss: 1.26735520362854\n",
      "epoch : 7 - step : 68/100 - loss: 1.1032614707946777\n",
      "epoch : 7 - step : 69/100 - loss: 1.1040791273117065\n",
      "epoch : 7 - step : 70/100 - loss: 0.9691144824028015\n",
      "epoch : 7 - step : 71/100 - loss: 0.895142674446106\n",
      "epoch : 7 - step : 72/100 - loss: 1.1089500188827515\n",
      "epoch : 7 - step : 73/100 - loss: 1.1191564798355103\n",
      "epoch : 7 - step : 74/100 - loss: 1.0170053243637085\n",
      "epoch : 7 - step : 75/100 - loss: 1.121700406074524\n",
      "epoch : 7 - step : 76/100 - loss: 1.027666449546814\n",
      "epoch : 7 - step : 77/100 - loss: 1.236832857131958\n",
      "epoch : 7 - step : 78/100 - loss: 0.9965867400169373\n",
      "epoch : 7 - step : 79/100 - loss: 0.865745484828949\n",
      "epoch : 7 - step : 80/100 - loss: 1.0198553800582886\n",
      "epoch : 7 - step : 81/100 - loss: 1.0852935314178467\n",
      "epoch : 7 - step : 82/100 - loss: 0.9245581030845642\n",
      "epoch : 7 - step : 83/100 - loss: 0.956357479095459\n",
      "epoch : 7 - step : 84/100 - loss: 1.0016577243804932\n",
      "epoch : 7 - step : 85/100 - loss: 0.9692265391349792\n",
      "epoch : 7 - step : 86/100 - loss: 0.9081448912620544\n",
      "epoch : 7 - step : 87/100 - loss: 0.9478827714920044\n",
      "epoch : 7 - step : 88/100 - loss: 1.0833916664123535\n",
      "epoch : 7 - step : 89/100 - loss: 1.0648586750030518\n",
      "epoch : 7 - step : 90/100 - loss: 0.9055160880088806\n",
      "epoch : 7 - step : 91/100 - loss: 1.1245330572128296\n",
      "epoch : 7 - step : 92/100 - loss: 0.7460255026817322\n",
      "epoch : 7 - step : 93/100 - loss: 1.2100797891616821\n",
      "epoch : 7 - step : 94/100 - loss: 0.7182416915893555\n",
      "epoch : 7 - step : 95/100 - loss: 1.1578216552734375\n",
      "epoch : 7 - step : 96/100 - loss: 1.1110738515853882\n",
      "epoch : 7 - step : 97/100 - loss: 1.166642665863037\n",
      "epoch : 7 - step : 98/100 - loss: 1.0474244356155396\n",
      "epoch : 7 - step : 99/100 - loss: 0.7922260761260986\n",
      "Epoch: 7, train_Loss:  1.0435799956321716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7/10 [43:42<18:45, 375.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Valid_Loss:  1.0677329325675964\n",
      "epoch : 8 - step : 0/100 - loss: 0.7220598459243774\n",
      "epoch : 8 - step : 1/100 - loss: 1.190659523010254\n",
      "epoch : 8 - step : 2/100 - loss: 0.6646177172660828\n",
      "epoch : 8 - step : 3/100 - loss: 0.9843756556510925\n",
      "epoch : 8 - step : 4/100 - loss: 1.1163620948791504\n",
      "epoch : 8 - step : 5/100 - loss: 1.0407356023788452\n",
      "epoch : 8 - step : 6/100 - loss: 1.3228981494903564\n",
      "epoch : 8 - step : 7/100 - loss: 0.9495016932487488\n",
      "epoch : 8 - step : 8/100 - loss: 1.1921871900558472\n",
      "epoch : 8 - step : 9/100 - loss: 0.8666924238204956\n",
      "epoch : 8 - step : 10/100 - loss: 0.9412743449211121\n",
      "epoch : 8 - step : 11/100 - loss: 1.122666835784912\n",
      "epoch : 8 - step : 12/100 - loss: 0.925174355506897\n",
      "epoch : 8 - step : 13/100 - loss: 0.8587319850921631\n",
      "epoch : 8 - step : 14/100 - loss: 0.979549765586853\n",
      "epoch : 8 - step : 15/100 - loss: 0.9644507169723511\n",
      "epoch : 8 - step : 16/100 - loss: 0.7420514822006226\n",
      "epoch : 8 - step : 17/100 - loss: 0.9479364156723022\n",
      "epoch : 8 - step : 18/100 - loss: 0.9089087843894958\n",
      "epoch : 8 - step : 19/100 - loss: 1.1002401113510132\n",
      "epoch : 8 - step : 20/100 - loss: 0.9258003830909729\n",
      "epoch : 8 - step : 21/100 - loss: 0.9622776508331299\n",
      "epoch : 8 - step : 22/100 - loss: 0.9415934681892395\n",
      "epoch : 8 - step : 23/100 - loss: 1.0565301179885864\n",
      "epoch : 8 - step : 24/100 - loss: 0.8781946897506714\n",
      "epoch : 8 - step : 25/100 - loss: 0.9645087122917175\n",
      "epoch : 8 - step : 26/100 - loss: 0.9452461004257202\n",
      "epoch : 8 - step : 27/100 - loss: 1.230445146560669\n",
      "epoch : 8 - step : 28/100 - loss: 0.8502190709114075\n",
      "epoch : 8 - step : 29/100 - loss: 1.0442891120910645\n",
      "epoch : 8 - step : 30/100 - loss: 0.9208094477653503\n",
      "epoch : 8 - step : 31/100 - loss: 0.9530043005943298\n",
      "epoch : 8 - step : 32/100 - loss: 1.1404237747192383\n",
      "epoch : 8 - step : 33/100 - loss: 1.0336304903030396\n",
      "epoch : 8 - step : 34/100 - loss: 1.0525463819503784\n",
      "epoch : 8 - step : 35/100 - loss: 0.9419596195220947\n",
      "epoch : 8 - step : 36/100 - loss: 0.5967827439308167\n",
      "epoch : 8 - step : 37/100 - loss: 1.248913049697876\n",
      "epoch : 8 - step : 38/100 - loss: 1.295691728591919\n",
      "epoch : 8 - step : 39/100 - loss: 0.926360547542572\n",
      "epoch : 8 - step : 40/100 - loss: 0.9428984522819519\n",
      "epoch : 8 - step : 41/100 - loss: 1.0323150157928467\n",
      "epoch : 8 - step : 42/100 - loss: 0.8161506056785583\n",
      "epoch : 8 - step : 43/100 - loss: 0.897262454032898\n",
      "epoch : 8 - step : 44/100 - loss: 0.9080251455307007\n",
      "epoch : 8 - step : 45/100 - loss: 0.8962990045547485\n",
      "epoch : 8 - step : 46/100 - loss: 1.2114664316177368\n",
      "epoch : 8 - step : 47/100 - loss: 0.9807437062263489\n",
      "epoch : 8 - step : 48/100 - loss: 0.8659228086471558\n",
      "epoch : 8 - step : 49/100 - loss: 0.9073019623756409\n",
      "epoch : 8 - step : 50/100 - loss: 0.9304426908493042\n",
      "epoch : 8 - step : 51/100 - loss: 1.1820405721664429\n",
      "epoch : 8 - step : 52/100 - loss: 1.0529998540878296\n",
      "epoch : 8 - step : 53/100 - loss: 0.983880341053009\n",
      "epoch : 8 - step : 54/100 - loss: 0.8950106501579285\n",
      "epoch : 8 - step : 55/100 - loss: 0.6948765516281128\n",
      "epoch : 8 - step : 56/100 - loss: 1.2198846340179443\n",
      "epoch : 8 - step : 57/100 - loss: 0.9581989049911499\n",
      "epoch : 8 - step : 58/100 - loss: 1.1953786611557007\n",
      "epoch : 8 - step : 59/100 - loss: 1.0592198371887207\n",
      "epoch : 8 - step : 60/100 - loss: 1.104779839515686\n",
      "epoch : 8 - step : 61/100 - loss: 0.9382150173187256\n",
      "epoch : 8 - step : 62/100 - loss: 1.167364478111267\n",
      "epoch : 8 - step : 63/100 - loss: 1.042703628540039\n",
      "epoch : 8 - step : 64/100 - loss: 1.0149685144424438\n",
      "epoch : 8 - step : 65/100 - loss: 1.0013638734817505\n",
      "epoch : 8 - step : 66/100 - loss: 1.0448143482208252\n",
      "epoch : 8 - step : 67/100 - loss: 0.7010369896888733\n",
      "epoch : 8 - step : 68/100 - loss: 1.0043516159057617\n",
      "epoch : 8 - step : 69/100 - loss: 0.7145019769668579\n",
      "epoch : 8 - step : 70/100 - loss: 1.0956038236618042\n",
      "epoch : 8 - step : 71/100 - loss: 1.0260474681854248\n",
      "epoch : 8 - step : 72/100 - loss: 0.9140485525131226\n",
      "epoch : 8 - step : 73/100 - loss: 0.9741275906562805\n",
      "epoch : 8 - step : 74/100 - loss: 0.7606135010719299\n",
      "epoch : 8 - step : 75/100 - loss: 0.7935926914215088\n",
      "epoch : 8 - step : 76/100 - loss: 1.0968550443649292\n",
      "epoch : 8 - step : 77/100 - loss: 1.0843385457992554\n",
      "epoch : 8 - step : 78/100 - loss: 0.8088815212249756\n",
      "epoch : 8 - step : 79/100 - loss: 0.9486904740333557\n",
      "epoch : 8 - step : 80/100 - loss: 1.1277012825012207\n",
      "epoch : 8 - step : 81/100 - loss: 1.0321033000946045\n",
      "epoch : 8 - step : 82/100 - loss: 1.014492154121399\n",
      "epoch : 8 - step : 83/100 - loss: 0.8851736187934875\n",
      "epoch : 8 - step : 84/100 - loss: 0.8467175960540771\n",
      "epoch : 8 - step : 85/100 - loss: 0.9623870849609375\n",
      "epoch : 8 - step : 86/100 - loss: 0.9995710849761963\n",
      "epoch : 8 - step : 87/100 - loss: 0.9591006636619568\n",
      "epoch : 8 - step : 88/100 - loss: 1.087820053100586\n",
      "epoch : 8 - step : 89/100 - loss: 1.0481873750686646\n",
      "epoch : 8 - step : 90/100 - loss: 0.8804307579994202\n",
      "epoch : 8 - step : 91/100 - loss: 1.0562188625335693\n",
      "epoch : 8 - step : 92/100 - loss: 1.0751521587371826\n",
      "epoch : 8 - step : 93/100 - loss: 1.173494815826416\n",
      "epoch : 8 - step : 94/100 - loss: 0.9437312483787537\n",
      "epoch : 8 - step : 95/100 - loss: 0.8913607001304626\n",
      "epoch : 8 - step : 96/100 - loss: 0.7506961822509766\n",
      "epoch : 8 - step : 97/100 - loss: 0.8101298809051514\n",
      "epoch : 8 - step : 98/100 - loss: 0.9088246822357178\n",
      "epoch : 8 - step : 99/100 - loss: 1.0999181270599365\n",
      "Epoch: 8, train_Loss:  0.9787073063850403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 8/10 [50:02<12:33, 376.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Valid_Loss:  1.0138829803466798\n",
      "epoch : 9 - step : 0/100 - loss: 1.0058503150939941\n",
      "epoch : 9 - step : 1/100 - loss: 1.0210704803466797\n",
      "epoch : 9 - step : 2/100 - loss: 0.889901876449585\n",
      "epoch : 9 - step : 3/100 - loss: 0.8693370819091797\n",
      "epoch : 9 - step : 4/100 - loss: 1.2519901990890503\n",
      "epoch : 9 - step : 5/100 - loss: 0.8669031858444214\n",
      "epoch : 9 - step : 6/100 - loss: 1.1830183267593384\n",
      "epoch : 9 - step : 7/100 - loss: 0.7014245390892029\n",
      "epoch : 9 - step : 8/100 - loss: 1.0002721548080444\n",
      "epoch : 9 - step : 9/100 - loss: 0.8426473736763\n",
      "epoch : 9 - step : 10/100 - loss: 0.9966603517532349\n",
      "epoch : 9 - step : 11/100 - loss: 1.106451153755188\n",
      "epoch : 9 - step : 12/100 - loss: 0.8935810923576355\n",
      "epoch : 9 - step : 13/100 - loss: 1.0383384227752686\n",
      "epoch : 9 - step : 14/100 - loss: 1.0647411346435547\n",
      "epoch : 9 - step : 15/100 - loss: 1.0439105033874512\n",
      "epoch : 9 - step : 16/100 - loss: 1.157841682434082\n",
      "epoch : 9 - step : 17/100 - loss: 1.152036190032959\n",
      "epoch : 9 - step : 18/100 - loss: 0.8068492412567139\n",
      "epoch : 9 - step : 19/100 - loss: 0.9235203862190247\n",
      "epoch : 9 - step : 20/100 - loss: 1.0910595655441284\n",
      "epoch : 9 - step : 21/100 - loss: 0.961625874042511\n",
      "epoch : 9 - step : 22/100 - loss: 0.9920827150344849\n",
      "epoch : 9 - step : 23/100 - loss: 1.193442702293396\n",
      "epoch : 9 - step : 24/100 - loss: 0.9767021536827087\n",
      "epoch : 9 - step : 25/100 - loss: 1.0666886568069458\n",
      "epoch : 9 - step : 26/100 - loss: 1.0914089679718018\n",
      "epoch : 9 - step : 27/100 - loss: 1.0972660779953003\n",
      "epoch : 9 - step : 28/100 - loss: 1.1276050806045532\n",
      "epoch : 9 - step : 29/100 - loss: 0.944961667060852\n",
      "epoch : 9 - step : 30/100 - loss: 0.9545742273330688\n",
      "epoch : 9 - step : 31/100 - loss: 1.0116393566131592\n",
      "epoch : 9 - step : 32/100 - loss: 0.9326766133308411\n",
      "epoch : 9 - step : 33/100 - loss: 1.1421023607254028\n",
      "epoch : 9 - step : 34/100 - loss: 1.0493230819702148\n",
      "epoch : 9 - step : 35/100 - loss: 1.119692087173462\n",
      "epoch : 9 - step : 36/100 - loss: 0.8576362133026123\n",
      "epoch : 9 - step : 37/100 - loss: 0.8253594040870667\n",
      "epoch : 9 - step : 38/100 - loss: 0.8513038158416748\n",
      "epoch : 9 - step : 39/100 - loss: 0.9819656610488892\n",
      "epoch : 9 - step : 40/100 - loss: 0.7388464212417603\n",
      "epoch : 9 - step : 41/100 - loss: 0.9824213981628418\n",
      "epoch : 9 - step : 42/100 - loss: 1.0654667615890503\n",
      "epoch : 9 - step : 43/100 - loss: 0.9434587359428406\n",
      "epoch : 9 - step : 44/100 - loss: 1.0537813901901245\n",
      "epoch : 9 - step : 45/100 - loss: 0.9864379167556763\n",
      "epoch : 9 - step : 46/100 - loss: 0.8383945822715759\n",
      "epoch : 9 - step : 47/100 - loss: 1.1324214935302734\n",
      "epoch : 9 - step : 48/100 - loss: 1.0456713438034058\n",
      "epoch : 9 - step : 49/100 - loss: 1.1014373302459717\n",
      "epoch : 9 - step : 50/100 - loss: 0.7371150255203247\n",
      "epoch : 9 - step : 51/100 - loss: 0.8295969367027283\n",
      "epoch : 9 - step : 52/100 - loss: 0.9217482209205627\n",
      "epoch : 9 - step : 53/100 - loss: 0.8597244620323181\n",
      "epoch : 9 - step : 54/100 - loss: 0.8212563395500183\n",
      "epoch : 9 - step : 55/100 - loss: 1.1603639125823975\n",
      "epoch : 9 - step : 56/100 - loss: 0.9582266807556152\n",
      "epoch : 9 - step : 57/100 - loss: 0.559380292892456\n",
      "epoch : 9 - step : 58/100 - loss: 0.7918758392333984\n",
      "epoch : 9 - step : 59/100 - loss: 0.6670462489128113\n",
      "epoch : 9 - step : 60/100 - loss: 1.0011931657791138\n",
      "epoch : 9 - step : 61/100 - loss: 0.6428748369216919\n",
      "epoch : 9 - step : 62/100 - loss: 0.9680296778678894\n",
      "epoch : 9 - step : 63/100 - loss: 0.911169707775116\n",
      "epoch : 9 - step : 64/100 - loss: 0.9028177857398987\n",
      "epoch : 9 - step : 65/100 - loss: 0.9044337272644043\n",
      "epoch : 9 - step : 66/100 - loss: 1.108719825744629\n",
      "epoch : 9 - step : 67/100 - loss: 0.7689077854156494\n",
      "epoch : 9 - step : 68/100 - loss: 1.123008131980896\n",
      "epoch : 9 - step : 69/100 - loss: 0.8351625204086304\n",
      "epoch : 9 - step : 70/100 - loss: 1.1923611164093018\n",
      "epoch : 9 - step : 71/100 - loss: 0.7771661877632141\n",
      "epoch : 9 - step : 72/100 - loss: 0.9714269638061523\n",
      "epoch : 9 - step : 73/100 - loss: 0.6760095357894897\n",
      "epoch : 9 - step : 74/100 - loss: 1.0818819999694824\n",
      "epoch : 9 - step : 75/100 - loss: 0.8457082509994507\n",
      "epoch : 9 - step : 76/100 - loss: 0.9558717012405396\n",
      "epoch : 9 - step : 77/100 - loss: 0.8576543927192688\n",
      "epoch : 9 - step : 78/100 - loss: 0.8811751008033752\n",
      "epoch : 9 - step : 79/100 - loss: 0.9632527232170105\n",
      "epoch : 9 - step : 80/100 - loss: 1.0321859121322632\n",
      "epoch : 9 - step : 81/100 - loss: 1.06817626953125\n",
      "epoch : 9 - step : 82/100 - loss: 1.0651891231536865\n",
      "epoch : 9 - step : 83/100 - loss: 0.9085232615470886\n",
      "epoch : 9 - step : 84/100 - loss: 0.8248904943466187\n",
      "epoch : 9 - step : 85/100 - loss: 0.9649537205696106\n",
      "epoch : 9 - step : 86/100 - loss: 0.9015343189239502\n",
      "epoch : 9 - step : 87/100 - loss: 0.783973217010498\n",
      "epoch : 9 - step : 88/100 - loss: 0.9742389917373657\n",
      "epoch : 9 - step : 89/100 - loss: 0.9160774350166321\n",
      "epoch : 9 - step : 90/100 - loss: 1.1022114753723145\n",
      "epoch : 9 - step : 91/100 - loss: 0.9456546902656555\n",
      "epoch : 9 - step : 92/100 - loss: 0.8592033982276917\n",
      "epoch : 9 - step : 93/100 - loss: 0.9902091026306152\n",
      "epoch : 9 - step : 94/100 - loss: 0.9985076785087585\n",
      "epoch : 9 - step : 95/100 - loss: 0.9159624576568604\n",
      "epoch : 9 - step : 96/100 - loss: 0.535934567451477\n",
      "epoch : 9 - step : 97/100 - loss: 0.9922828078269958\n",
      "epoch : 9 - step : 98/100 - loss: 1.0119084119796753\n",
      "epoch : 9 - step : 99/100 - loss: 1.0126756429672241\n",
      "Epoch: 9, train_Loss:  0.9545325142145157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 9/10 [56:15<06:15, 375.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Valid_Loss:  0.9707102751731873\n",
      "epoch : 10 - step : 0/100 - loss: 0.9551087617874146\n",
      "epoch : 10 - step : 1/100 - loss: 0.9268535375595093\n",
      "epoch : 10 - step : 2/100 - loss: 0.8697043657302856\n",
      "epoch : 10 - step : 3/100 - loss: 0.816990077495575\n",
      "epoch : 10 - step : 4/100 - loss: 0.8447631001472473\n",
      "epoch : 10 - step : 5/100 - loss: 0.9637778997421265\n",
      "epoch : 10 - step : 6/100 - loss: 0.5477986931800842\n",
      "epoch : 10 - step : 7/100 - loss: 0.970766007900238\n",
      "epoch : 10 - step : 8/100 - loss: 0.9488482475280762\n",
      "epoch : 10 - step : 9/100 - loss: 0.8797046542167664\n",
      "epoch : 10 - step : 10/100 - loss: 0.9117159843444824\n",
      "epoch : 10 - step : 11/100 - loss: 0.8255541324615479\n",
      "epoch : 10 - step : 12/100 - loss: 0.8005479574203491\n",
      "epoch : 10 - step : 13/100 - loss: 0.7647018432617188\n",
      "epoch : 10 - step : 14/100 - loss: 0.9565283060073853\n",
      "epoch : 10 - step : 15/100 - loss: 0.9900828003883362\n",
      "epoch : 10 - step : 16/100 - loss: 0.8712703585624695\n",
      "epoch : 10 - step : 17/100 - loss: 0.8944891691207886\n",
      "epoch : 10 - step : 18/100 - loss: 0.9250288009643555\n",
      "epoch : 10 - step : 19/100 - loss: 0.9517401456832886\n",
      "epoch : 10 - step : 20/100 - loss: 0.8362072706222534\n",
      "epoch : 10 - step : 21/100 - loss: 1.0233519077301025\n",
      "epoch : 10 - step : 22/100 - loss: 0.8405311107635498\n",
      "epoch : 10 - step : 23/100 - loss: 1.0611602067947388\n",
      "epoch : 10 - step : 24/100 - loss: 0.9973900318145752\n",
      "epoch : 10 - step : 25/100 - loss: 0.8843042254447937\n",
      "epoch : 10 - step : 26/100 - loss: 0.8596217632293701\n",
      "epoch : 10 - step : 27/100 - loss: 0.7665110230445862\n",
      "epoch : 10 - step : 28/100 - loss: 0.8044542670249939\n",
      "epoch : 10 - step : 29/100 - loss: 0.784894585609436\n",
      "epoch : 10 - step : 30/100 - loss: 1.037111520767212\n",
      "epoch : 10 - step : 31/100 - loss: 0.9352849721908569\n",
      "epoch : 10 - step : 32/100 - loss: 0.7506440877914429\n",
      "epoch : 10 - step : 33/100 - loss: 1.123138189315796\n",
      "epoch : 10 - step : 34/100 - loss: 0.853752076625824\n",
      "epoch : 10 - step : 35/100 - loss: 0.913043737411499\n",
      "epoch : 10 - step : 36/100 - loss: 0.8083174824714661\n",
      "epoch : 10 - step : 37/100 - loss: 1.0284572839736938\n",
      "epoch : 10 - step : 38/100 - loss: 1.0242340564727783\n",
      "epoch : 10 - step : 39/100 - loss: 0.9849373698234558\n",
      "epoch : 10 - step : 40/100 - loss: 0.9424587488174438\n",
      "epoch : 10 - step : 41/100 - loss: 0.9745470285415649\n",
      "epoch : 10 - step : 42/100 - loss: 1.0288586616516113\n",
      "epoch : 10 - step : 43/100 - loss: 0.7841687798500061\n",
      "epoch : 10 - step : 44/100 - loss: 0.9909300804138184\n",
      "epoch : 10 - step : 45/100 - loss: 1.0018917322158813\n",
      "epoch : 10 - step : 46/100 - loss: 0.971672534942627\n",
      "epoch : 10 - step : 47/100 - loss: 1.0949225425720215\n",
      "epoch : 10 - step : 48/100 - loss: 0.8963661193847656\n",
      "epoch : 10 - step : 49/100 - loss: 0.9595872759819031\n",
      "epoch : 10 - step : 50/100 - loss: 1.0546993017196655\n",
      "epoch : 10 - step : 51/100 - loss: 0.865732729434967\n",
      "epoch : 10 - step : 52/100 - loss: 1.2264553308486938\n",
      "epoch : 10 - step : 53/100 - loss: 0.9339096546173096\n",
      "epoch : 10 - step : 54/100 - loss: 0.9364079833030701\n",
      "epoch : 10 - step : 55/100 - loss: 0.7922484874725342\n",
      "epoch : 10 - step : 56/100 - loss: 0.948689341545105\n",
      "epoch : 10 - step : 57/100 - loss: 1.1256604194641113\n",
      "epoch : 10 - step : 58/100 - loss: 0.8061770796775818\n",
      "epoch : 10 - step : 59/100 - loss: 1.0052211284637451\n",
      "epoch : 10 - step : 60/100 - loss: 1.042555809020996\n",
      "epoch : 10 - step : 61/100 - loss: 1.1377109289169312\n",
      "epoch : 10 - step : 62/100 - loss: 0.6295745372772217\n",
      "epoch : 10 - step : 63/100 - loss: 0.7565003633499146\n",
      "epoch : 10 - step : 64/100 - loss: 0.8863881826400757\n",
      "epoch : 10 - step : 65/100 - loss: 0.909765362739563\n",
      "epoch : 10 - step : 66/100 - loss: 0.7048014998435974\n",
      "epoch : 10 - step : 67/100 - loss: 0.6137242317199707\n",
      "epoch : 10 - step : 68/100 - loss: 1.096451997756958\n",
      "epoch : 10 - step : 69/100 - loss: 0.9625855684280396\n",
      "epoch : 10 - step : 70/100 - loss: 1.0500972270965576\n",
      "epoch : 10 - step : 71/100 - loss: 0.8901991844177246\n",
      "epoch : 10 - step : 72/100 - loss: 0.967309296131134\n",
      "epoch : 10 - step : 73/100 - loss: 0.8936165571212769\n",
      "epoch : 10 - step : 74/100 - loss: 1.0474892854690552\n",
      "epoch : 10 - step : 75/100 - loss: 0.9404674768447876\n",
      "epoch : 10 - step : 76/100 - loss: 1.0122549533843994\n",
      "epoch : 10 - step : 77/100 - loss: 1.1371798515319824\n",
      "epoch : 10 - step : 78/100 - loss: 1.1218656301498413\n",
      "epoch : 10 - step : 79/100 - loss: 0.6339839696884155\n",
      "epoch : 10 - step : 80/100 - loss: 1.025925636291504\n",
      "epoch : 10 - step : 81/100 - loss: 0.822938859462738\n",
      "epoch : 10 - step : 82/100 - loss: 0.9732815623283386\n",
      "epoch : 10 - step : 83/100 - loss: 0.9771875739097595\n",
      "epoch : 10 - step : 84/100 - loss: 0.8901615738868713\n",
      "epoch : 10 - step : 85/100 - loss: 0.790743887424469\n",
      "epoch : 10 - step : 86/100 - loss: 0.9810486435890198\n",
      "epoch : 10 - step : 87/100 - loss: 0.9196226000785828\n",
      "epoch : 10 - step : 88/100 - loss: 0.856037974357605\n",
      "epoch : 10 - step : 89/100 - loss: 0.9743724465370178\n",
      "epoch : 10 - step : 90/100 - loss: 0.939058780670166\n",
      "epoch : 10 - step : 91/100 - loss: 0.8509494066238403\n",
      "epoch : 10 - step : 92/100 - loss: 0.9928267002105713\n",
      "epoch : 10 - step : 93/100 - loss: 0.8568704128265381\n",
      "epoch : 10 - step : 94/100 - loss: 0.8379432559013367\n",
      "epoch : 10 - step : 95/100 - loss: 0.9484016299247742\n",
      "epoch : 10 - step : 96/100 - loss: 0.9748502969741821\n",
      "epoch : 10 - step : 97/100 - loss: 0.8893749117851257\n",
      "epoch : 10 - step : 98/100 - loss: 1.1259982585906982\n",
      "epoch : 10 - step : 99/100 - loss: 0.9841026067733765\n",
      "Epoch: 10, train_Loss:  0.9232014590501785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [1:02:28<00:00, 374.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Valid_Loss:  0.9545117878913879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "loss_dic = {\"epoch\":[],\"Train\":[], \"Val\":[]}\n",
    "best_loss = 100\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in tqdm(range(1, 11)):\n",
    "    \n",
    "    loss_dic['epoch'].append(epoch)\n",
    "    train(epoch, train_loader)\n",
    "    validate(epoch, valid_loader)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if loss_dic['Val'][epoch - 1] > best_loss:\n",
    "        early_stop_count += 1       \n",
    "        if early_stop_count >= 2:\n",
    "            loss_dic_df = pd.DataFrame(loss_dic)\n",
    "            loss_dic_df.to_csv(f'./results/base_loss_epoch{epoch}.csv', index=False)\n",
    "            torch.save(model.state_dict(), f'./savedmodel/base_bestmodel_epoch{epoch}.pth')\n",
    "            break\n",
    "    else:\n",
    "        best_loss = loss_dic['Val'][epoch - 1]\n",
    "        early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5301ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping 안됐을때 모델, 결과 따로 저장\n",
    "torch.save(model.state_dict(), './savedmodel/base_bestmodel_epoch10.pth')\n",
    "loss_dic_df = pd.DataFrame(loss_dic)\n",
    "loss_dic_df.to_csv('./results/base_loss_epoch10.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ca3ea",
   "metadata": {},
   "source": [
    "# 추론 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "239a64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a87be34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig,HfArgumentParser, get_scheduler, set_seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e7fdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'mode_ID':\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "          'seed': 1 ,\n",
    "          'max_seq_len' : 4096,\n",
    "          'epochs': 3,\n",
    "          'lr': 2e-4,\n",
    "          'batch': 4,\n",
    "          'lora_r':8,\n",
    "          'lora_alpha':32,\n",
    "          'target_module':[\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\",\"gate_proj\", \"v_proj\"],\n",
    "          'lora_dropout':0.05,\n",
    "          'lora_tasktype' :'CAUSAL_LM',\n",
    "          'lora_bias' : 'none',\n",
    "          'optimizer': 'paged_adamw_8bit',\n",
    "          'scheduler':'cosine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c6b9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,  # PEFT 설정을 가져오기 위한 함수\n",
    "    get_peft_model,  # PEFT 모델을 가져오기 위한 함수\n",
    "    get_peft_model_state_dict,  # PEFT 모델 상태 사전을 가져오기 위한 함수\n",
    "    set_peft_model_state_dict,  # PEFT 모델 상태 사전을 설정하기 위한 함수\n",
    "    LoraConfig,  # LoRA 모델 구성을 정의하는 클래스\n",
    "    PeftType,  # PEFT 모델의 타입을 정의\n",
    "    PrefixTuningConfig,  # PrefixTuning 모델 구성을 정의하는 클래스\n",
    "    PromptEncoderConfig,  # PromptEncoder 모델 구성을 정의하는 클래스\n",
    "    PeftModel,  # PEFT 모델을 정의하는 클래스\n",
    "    PeftConfig,  # PEFT 모델의 구성을 정의하는 클래스\n",
    ")\n",
    "\n",
    "# PEFT 모델의 타입 설정 (LoRA로 설정)\n",
    "peft_type = PeftType.LORA\n",
    "\n",
    "# LoRA 모델을 위한 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=config['lora_r'],  # LoRA 모델의 r 값\n",
    "    lora_alpha=config['lora_alpha'],  # LoRA 모델의 alpha 값\n",
    "    target_modules=config['target_module'],  # LoRA 모델의 타겟 모듈 리스트\n",
    "    lora_dropout=config['lora_dropout'],  # LoRA 모델의 드롭아웃 비율\n",
    "    bias=config['lora_bias'],  # LoRA 모델의 편향 설정\n",
    "    task_type=config['lora_tasktype']  # LoRA 모델의 태스크 유형\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bed0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.73s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig['mode_ID'],\n",
    "\tdevice_map=\"cuda\",\n",
    "\ttorch_dtype=torch.float16,\n",
    "\ttrust_remote_code=True, \n",
    "\tuse_cache=False,\n",
    "    # attn_implementation='flash_attention_2'\n",
    "\t# quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c155e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc9857b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config) # PEFT 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de21425b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./savedmodel/base_bestmodel_epoch10.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d9c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer를 사용하여 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['mode_ID'], trust_remote_code=True, eos_token='</s>')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def make_prompt(user_request, answer):\n",
    "    \n",
    "    conversation = [ {'role': 'user', 'content': user_request},\n",
    "                  {'role': 'assistant', 'content': answer}]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "with open('./data/pqaa_train_set.json','r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('./data/pqaa_dev_set.json','r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "# 데이터프레임에 넣을 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 딕셔너리를 순회하며 데이터프레임용 리스트 생성\n",
    "for num, details in test_data.items():\n",
    "    contexts_with_labels = '\\n'.join([f\"({label}) {context}\" for label, context in zip(details['LABELS'], details['CONTEXTS'])])\n",
    "    input = 'Question:\\n' + details['QUESTION'] + '\\nPlease give me the answer in formats: yes or no' + '\\n' + 'Context:\\n' + contexts_with_labels\n",
    "    row = {\n",
    "        'input' : input,\n",
    "        'final_decision': details['final_decision']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=50, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=50, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df_test = pd.concat([no_df, yes_df])\n",
    "\n",
    "# print(combined_df_test)\n",
    "\n",
    "X_test = combined_df_test['input']\n",
    "y_test = combined_df_test['final_decision']\n",
    "\n",
    "test_data_prompt_list = []\n",
    "for x3,y3 in zip(X_test, y_test):\n",
    "    test_data_prompt_list.append(make_prompt(x3,y3))\n",
    "    test_data_prompt_list = [test_data.split('<|end|>')[0] + '<|end|>\\n<|assistant|>\\n' for test_data in test_data_prompt_list]\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a001c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(test_data_prompt_list)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8cef43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "def test(loader):\n",
    "    output_li = []\n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "\n",
    "    pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    ) \n",
    "\n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 500, \n",
    "        \"return_full_text\": False, \n",
    "        \"temperature\": 0.5, \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for output in tqdm(pipe(loader, **generation_args)):\n",
    "            # inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            # outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            output_li.append(output)\n",
    "            \n",
    "    #         loss = outputs.loss\n",
    "    #         loss_avg += loss.item()\n",
    "            \n",
    "    #         del inputs\n",
    "    #         del outputs\n",
    "    #         del loss\n",
    "            \n",
    "    # print(f'Epoch: {epoch}, Valid_Loss:  {loss_avg/len(loader)}')\n",
    "    # loss_dic['Val'].append(loss_avg/len(loader))\n",
    "    return output_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6943f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'Phi3ForCausalLM'].\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "100%|██████████| 100/100 [00:39<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' no'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = test(test_dataset)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3569db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_li = []\n",
    "for output in outputs:\n",
    "    # print(output[0].get('generated_text').strip())\n",
    "    pred_li.append(output[0].get('generated_text').strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "19b5fc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7219</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true pred\n",
       "2774    no   no\n",
       "4083    no  yes\n",
       "10063   no   no\n",
       "3060    no   no\n",
       "7219    no   no\n",
       "...    ...  ...\n",
       "1469   yes   no\n",
       "3216   yes  yes\n",
       "8590   yes  yes\n",
       "4770   yes  yes\n",
       "6134   yes  yes\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'true': y_test, 'pred':pred_li})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef06f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df['true'],df['pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada94d3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.07/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
