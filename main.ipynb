{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e9267e0-1905-4e0a-90b0-63e7813128b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주요 파라미터\n",
    "\n",
    "class config():\n",
    "    def __init__(self):\n",
    "        self.seed = 600  # 시드 값 설정\n",
    "        self.max_len = 2048  # 입력 시퀀스의 최대 길이\n",
    "        self.epochs = 3  # 학습 에폭 수\n",
    "        self.learning_rate = 2e-4  # 학습률 설정\n",
    "        self.batch_size = 4  # 배치 크기\n",
    "\n",
    "        self.lora_r = 8  # LoRA 모델 파라미터: r 값\n",
    "        self.lora_alpha = 32  # LoRA 모델 파라미터: alpha 값\n",
    "        self.target_module = [\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\",\n",
    "                              \"gate_proj\", \"v_proj\"]  # 타겟 모듈 리스트\n",
    "        self.lora_dropout = 0.05  # LoRA 모델 파라미터: 드롭아웃 비율\n",
    "        self.lora_tasktype = \"CAUSAL_LM\"  # LoRA 모델 파라미터: 태스크 유형\n",
    "        self.lora_bias = 'none'  # LoRA 모델 파라미터: 편향 설정\n",
    "        self.optimizer = \"paged_adamw_8bit\"  # 옵티마이저 종류\n",
    "        self.scheduler = \"cosine\"  # 스케줄러 종류\n",
    "\n",
    "# config 클래스의 인스턴스 생성\n",
    "cfg = config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,  # PEFT 설정을 가져오기 위한 함수\n",
    "    get_peft_model,  # PEFT 모델을 가져오기 위한 함수\n",
    "    get_peft_model_state_dict,  # PEFT 모델 상태 사전을 가져오기 위한 함수\n",
    "    set_peft_model_state_dict,  # PEFT 모델 상태 사전을 설정하기 위한 함수\n",
    "    LoraConfig,  # LoRA 모델 구성을 정의하는 클래스\n",
    "    PeftType,  # PEFT 모델의 타입을 정의\n",
    "    PrefixTuningConfig,  # PrefixTuning 모델 구성을 정의하는 클래스\n",
    "    PromptEncoderConfig,  # PromptEncoder 모델 구성을 정의하는 클래스\n",
    "    PeftModel,  # PEFT 모델을 정의하는 클래스\n",
    "    PeftConfig,  # PEFT 모델의 구성을 정의하는 클래스\n",
    ")\n",
    "\n",
    "# PEFT 모델의 타입 설정 (LoRA로 설정)\n",
    "peft_type = PeftType.LORA\n",
    "\n",
    "# LoRA 모델을 위한 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=cfg.lora_r,  # LoRA 모델의 r 값\n",
    "    lora_alpha=cfg.lora_alpha,  # LoRA 모델의 alpha 값\n",
    "    target_modules=cfg.target_module,  # LoRA 모델의 타겟 모듈 리스트\n",
    "    lora_dropout=cfg.lora_dropout,  # LoRA 모델의 드롭아웃 비율\n",
    "    bias=cfg.lora_bias,  # LoRA 모델의 편향 설정\n",
    "    task_type=cfg.lora_tasktype  # LoRA 모델의 태스크 유형\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed05f2fd-0929-4c62-8b7e-2c26eb3b0b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.04s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model_ID = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tmodel_ID,\n",
    "\tdevice_map=\"cuda\",\n",
    "\ttorch_dtype=torch.float16,\n",
    "\ttrust_remote_code=True, \n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# AutoTokenizer를 사용하여 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_path, trust_remote_code=True, eos_token='</s>')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.1165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): PeftModelForCausalLM(\n",
       "      (base_model): LoraModel(\n",
       "        (model): PeftModelForCausalLM(\n",
       "          (base_model): LoraModel(\n",
       "            (model): Phi3ForCausalLM(\n",
       "              (model): Phi3Model(\n",
       "                (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n",
       "                (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "                (layers): ModuleList(\n",
       "                  (0-31): 32 x Phi3DecoderLayer(\n",
       "                    (self_attn): Phi3Attention(\n",
       "                      (o_proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=3072, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (qkv_proj): Linear(in_features=3072, out_features=9216, bias=False)\n",
       "                      (rotary_emb): Phi3RotaryEmbedding()\n",
       "                    )\n",
       "                    (mlp): Phi3MLP(\n",
       "                      (gate_up_proj): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "                      (down_proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Dropout(p=0.05, inplace=False)\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (activation_fn): SiLU()\n",
       "                    )\n",
       "                    (input_layernorm): Phi3RMSNorm()\n",
       "                    (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
       "                    (post_attention_layernorm): Phi3RMSNorm()\n",
       "                  )\n",
       "                )\n",
       "                (norm): Phi3RMSNorm()\n",
       "              )\n",
       "              (lm_head): Linear(in_features=3072, out_features=32064, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# peft 라이브러리에서 k 비트 학습 준비 함수 임포트\n",
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "# 모델에서 그래디언트 체크포인팅 활성화 (메모리 효율 향상)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# k 비트 학습을 위해 모델 준비 - prepare_model_for_kbit_training 함수 사용\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# PEFT 적용 \n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# 모델을 학습 장치 (GPU 등)로 이동\n",
    "model = model.to(device)\n",
    "\n",
    "# 훈련 가능한 파라미터 출력 \n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 모델 출력\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23758421-c94e-463a-a25b-048854782cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(user_request, answer):\n",
    "    \n",
    "    conversation = [ {'role': 'user', 'content': user_request},\n",
    "                  {'role': 'assistant', 'content': answer}]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 233M/233M [00:12<00:00, 18.6MB/s] \n",
      "Generating train split: 100%|██████████| 211269/211269 [00:01<00:00, 116801.64 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('qiaojin/PubMedQA', 'pqa_artificial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = dataset['train']['question'].to_frame()\n",
    "c = dataset['train']['context'].to_frame()\n",
    "label = dataset['train']['final_decision'].to_frame()\n",
    "df_all = pd.concat([q,c,label], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>final_decision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are group 2 innate lymphoid cells ( ILC2s ) in...</td>\n",
       "      <td>{'contexts': ['Chronic rhinosinusitis (CRS) is...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Does vagus nerve contribute to the development...</td>\n",
       "      <td>{'contexts': ['Phosphatidylethanolamine N-meth...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Does psammaplin A induce Sirtuin 1-dependent a...</td>\n",
       "      <td>{'contexts': ['Psammaplin A (PsA) is a natural...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is methylation of the FGFR2 gene associated wi...</td>\n",
       "      <td>{'contexts': ['This study examined links betwe...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Do tumor-infiltrating immune cell profiles and...</td>\n",
       "      <td>{'contexts': ['Tumor microenvironment immunity...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211264</th>\n",
       "      <td>Is urine production rate related to behavioura...</td>\n",
       "      <td>{'contexts': ['To investigate the relation bet...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211265</th>\n",
       "      <td>Does evaluation of the use of general practice...</td>\n",
       "      <td>{'contexts': ['This study set out to show how ...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211266</th>\n",
       "      <td>Does intracoronary angiotensin-converting enzy...</td>\n",
       "      <td>{'contexts': ['There is increasing recognition...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211267</th>\n",
       "      <td>Does transfusion significantly increase the ri...</td>\n",
       "      <td>{'contexts': ['To determine if splenectomy res...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211268</th>\n",
       "      <td>Is low intramucosal pH associated with failure...</td>\n",
       "      <td>{'contexts': ['To determine if low gastric int...</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211269 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 question  \\\n",
       "0       Are group 2 innate lymphoid cells ( ILC2s ) in...   \n",
       "1       Does vagus nerve contribute to the development...   \n",
       "2       Does psammaplin A induce Sirtuin 1-dependent a...   \n",
       "3       Is methylation of the FGFR2 gene associated wi...   \n",
       "4       Do tumor-infiltrating immune cell profiles and...   \n",
       "...                                                   ...   \n",
       "211264  Is urine production rate related to behavioura...   \n",
       "211265  Does evaluation of the use of general practice...   \n",
       "211266  Does intracoronary angiotensin-converting enzy...   \n",
       "211267  Does transfusion significantly increase the ri...   \n",
       "211268  Is low intramucosal pH associated with failure...   \n",
       "\n",
       "                                                  context final_decision  \n",
       "0       {'contexts': ['Chronic rhinosinusitis (CRS) is...            yes  \n",
       "1       {'contexts': ['Phosphatidylethanolamine N-meth...            yes  \n",
       "2       {'contexts': ['Psammaplin A (PsA) is a natural...            yes  \n",
       "3       {'contexts': ['This study examined links betwe...            yes  \n",
       "4       {'contexts': ['Tumor microenvironment immunity...            yes  \n",
       "...                                                   ...            ...  \n",
       "211264  {'contexts': ['To investigate the relation bet...            yes  \n",
       "211265  {'contexts': ['This study set out to show how ...            yes  \n",
       "211266  {'contexts': ['There is increasing recognition...            yes  \n",
       "211267  {'contexts': ['To determine if splenectomy res...            yes  \n",
       "211268  {'contexts': ['To determine if low gastric int...            yes  \n",
       "\n",
       "[211269 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2d9322c-1c41-4c96-82d7-0159d54f37a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_excel('../train_data.xlsx')\n",
    "valid_df = pd.read_excel('../valid_data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6abfa3b4-c0fd-494e-a825-abaca4e5497f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "No chat template is defined for this tokenizer - using the default template for the LlamaTokenizerFast class. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data_prompt_list = []\n",
    "for i,row in train_df.iterrows():\n",
    "    train_data_prompt_list.append(make_prompt(row['instruct'], row['answer']))\n",
    "\n",
    "valid_data_prompt_list = []\n",
    "for i,row in valid_df.iterrows():\n",
    "    valid_data_prompt_list.append(make_prompt(row['instruct'], row['answer']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d05d3a2-d9ee-43f2-9f40-980669abd577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f92546-0270-4385-b977-f75f3892c640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data_prompt_list)\n",
    "valid_dataset = Dataset(valid_data_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "554587bf-a69f-423d-abf1-7a3cbe77746d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, loader):\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    for i, prompt in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        scaler.update()\n",
    "        print(f\"epoch : {epoch} - step : {i}/{len(loader)} - loss: {loss.item()}\")\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "        del inputs\n",
    "        del outputs\n",
    "        del loss\n",
    "        \n",
    "    print(f'Epoch: {epoch}, train_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Train'].append(loss_avg/len(loader))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd12d81f-d0a5-4694-9971-504645178cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch,loader):  \n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    with torch.no_grad():       \n",
    "        for i, prompt in enumerate(loader):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss_avg += loss.item()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            del loss\n",
    "            \n",
    "    print(f'Epoch: {epoch}, Valid_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Val'].append(loss_avg/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35f933dc-9b98-42ed-88b4-1cf78e7ed602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184ee7b8-d2bf-4153-bea4-bd29fc30bd55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optimizer = AdamW(model.parameters(), lr = 1e-5)\n",
    "optimizer = SGD(model.parameters(), lr=1e-4)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698da1f7-7dee-4441-b0ce-fc776c6ff75c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4791b-bdab-4cf0-b465-e66be73c1ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 1 - step : 0/14 - loss: 3.338663339614868\n",
      "epoch : 1 - step : 1/14 - loss: 3.2922987937927246\n",
      "epoch : 1 - step : 2/14 - loss: 3.3386213779449463\n",
      "epoch : 1 - step : 3/14 - loss: 3.3227198123931885\n",
      "epoch : 1 - step : 4/14 - loss: 3.4006810188293457\n",
      "epoch : 1 - step : 5/14 - loss: 3.3081371784210205\n",
      "epoch : 1 - step : 6/14 - loss: 3.4179751873016357\n",
      "epoch : 1 - step : 7/14 - loss: 2.0940959453582764\n",
      "epoch : 1 - step : 8/14 - loss: 2.1034910678863525\n",
      "epoch : 1 - step : 9/14 - loss: 1.8338282108306885\n",
      "epoch : 1 - step : 10/14 - loss: 1.859686255455017\n",
      "epoch : 1 - step : 11/14 - loss: 1.6385167837142944\n",
      "epoch : 1 - step : 12/14 - loss: 1.6211540699005127\n",
      "epoch : 1 - step : 13/14 - loss: 1.7495423555374146\n",
      "Epoch: 1, train_Loss:  2.5942436712128774\n",
      "Epoch: 1, Valid_Loss:  1.6565044522285461\n",
      "epoch : 2 - step : 0/14 - loss: 1.6129189729690552\n",
      "epoch : 2 - step : 1/14 - loss: 1.5143346786499023\n",
      "epoch : 2 - step : 2/14 - loss: 1.4992808103561401\n",
      "epoch : 2 - step : 3/14 - loss: 1.5427643060684204\n",
      "epoch : 2 - step : 4/14 - loss: 1.5815447568893433\n",
      "epoch : 2 - step : 5/14 - loss: 1.5890411138534546\n",
      "epoch : 2 - step : 6/14 - loss: 1.4626045227050781\n",
      "epoch : 2 - step : 7/14 - loss: 1.4360319375991821\n",
      "epoch : 2 - step : 8/14 - loss: 1.3447799682617188\n",
      "epoch : 2 - step : 9/14 - loss: 1.2941280603408813\n",
      "epoch : 2 - step : 10/14 - loss: 1.2279518842697144\n",
      "epoch : 2 - step : 11/14 - loss: 1.2181957960128784\n",
      "epoch : 2 - step : 12/14 - loss: 0.918415367603302\n",
      "epoch : 2 - step : 13/14 - loss: 0.9823958277702332\n",
      "Epoch: 2, train_Loss:  1.3731705716678075\n",
      "Epoch: 2, Valid_Loss:  0.949328139424324\n",
      "epoch : 3 - step : 0/14 - loss: 1.0395684242248535\n",
      "epoch : 3 - step : 1/14 - loss: 0.8940079808235168\n",
      "epoch : 3 - step : 2/14 - loss: 0.9235824942588806\n",
      "epoch : 3 - step : 3/14 - loss: 0.8288651704788208\n",
      "epoch : 3 - step : 4/14 - loss: 0.662867546081543\n",
      "epoch : 3 - step : 5/14 - loss: 0.8559644222259521\n",
      "epoch : 3 - step : 6/14 - loss: 0.7542946934700012\n",
      "epoch : 3 - step : 7/14 - loss: 0.828682541847229\n",
      "epoch : 3 - step : 8/14 - loss: 0.7763330936431885\n",
      "epoch : 3 - step : 9/14 - loss: 0.8097240328788757\n",
      "epoch : 3 - step : 10/14 - loss: 0.7436689734458923\n",
      "epoch : 3 - step : 11/14 - loss: 0.8066731691360474\n",
      "epoch : 3 - step : 12/14 - loss: 0.761597752571106\n",
      "epoch : 3 - step : 13/14 - loss: 0.8346095085144043\n",
      "Epoch: 3, train_Loss:  0.8228885574000222\n",
      "Epoch: 3, Valid_Loss:  0.8012906163930893\n",
      "epoch : 4 - step : 0/14 - loss: 0.7212182283401489\n",
      "epoch : 4 - step : 1/14 - loss: 0.7706011533737183\n",
      "epoch : 4 - step : 2/14 - loss: 0.626693069934845\n",
      "epoch : 4 - step : 3/14 - loss: 0.7011995911598206\n",
      "epoch : 4 - step : 4/14 - loss: 0.6967843174934387\n",
      "epoch : 4 - step : 5/14 - loss: 0.7293658256530762\n",
      "epoch : 4 - step : 6/14 - loss: 0.7898341417312622\n",
      "epoch : 4 - step : 7/14 - loss: 0.8405824899673462\n",
      "epoch : 4 - step : 8/14 - loss: 0.8026133179664612\n",
      "epoch : 4 - step : 9/14 - loss: 0.7021098136901855\n",
      "epoch : 4 - step : 10/14 - loss: 0.6842160820960999\n",
      "epoch : 4 - step : 11/14 - loss: 0.7914147973060608\n",
      "epoch : 4 - step : 12/14 - loss: 0.8396263718605042\n",
      "epoch : 4 - step : 13/14 - loss: 0.802723228931427\n",
      "Epoch: 4, train_Loss:  0.749927316393171\n",
      "Epoch: 4, Valid_Loss:  0.7710955142974854\n",
      "epoch : 5 - step : 0/14 - loss: 0.7203034162521362\n",
      "epoch : 5 - step : 1/14 - loss: 0.6887889504432678\n",
      "epoch : 5 - step : 2/14 - loss: 0.7578770518302917\n",
      "epoch : 5 - step : 3/14 - loss: 0.6501343250274658\n",
      "epoch : 5 - step : 4/14 - loss: 0.6771592497825623\n",
      "epoch : 5 - step : 5/14 - loss: 0.8230485320091248\n",
      "epoch : 5 - step : 6/14 - loss: 0.7528666257858276\n",
      "epoch : 5 - step : 7/14 - loss: 0.6565365195274353\n",
      "epoch : 5 - step : 8/14 - loss: 0.5968919992446899\n",
      "epoch : 5 - step : 9/14 - loss: 0.8592622876167297\n",
      "epoch : 5 - step : 10/14 - loss: 0.8301762938499451\n",
      "epoch : 5 - step : 11/14 - loss: 0.7704155445098877\n",
      "epoch : 5 - step : 12/14 - loss: 0.7762442231178284\n",
      "epoch : 5 - step : 13/14 - loss: 0.7607303261756897\n",
      "Epoch: 5, train_Loss:  0.7371739532266345\n",
      "Epoch: 5, Valid_Loss:  0.7707269489765167\n",
      "epoch : 6 - step : 0/14 - loss: 0.754748523235321\n",
      "epoch : 6 - step : 1/14 - loss: 0.6890983581542969\n",
      "epoch : 6 - step : 2/14 - loss: 0.6162521243095398\n",
      "epoch : 6 - step : 3/14 - loss: 0.6330011487007141\n",
      "epoch : 6 - step : 4/14 - loss: 0.6908080577850342\n",
      "epoch : 6 - step : 5/14 - loss: 0.6642575263977051\n",
      "epoch : 6 - step : 6/14 - loss: 0.6494216918945312\n",
      "epoch : 6 - step : 7/14 - loss: 0.83095383644104\n",
      "epoch : 6 - step : 8/14 - loss: 0.6361353993415833\n",
      "epoch : 6 - step : 9/14 - loss: 0.8051018118858337\n",
      "epoch : 6 - step : 10/14 - loss: 0.7374507188796997\n",
      "epoch : 6 - step : 11/14 - loss: 0.7823035717010498\n",
      "epoch : 6 - step : 12/14 - loss: 0.734408974647522\n",
      "epoch : 6 - step : 13/14 - loss: 0.747035562992096\n",
      "Epoch: 6, train_Loss:  0.7122126647404262\n",
      "Epoch: 6, Valid_Loss:  0.7511106580495834\n",
      "epoch : 7 - step : 0/14 - loss: 0.6720407605171204\n",
      "epoch : 7 - step : 1/14 - loss: 0.7733921408653259\n",
      "epoch : 7 - step : 2/14 - loss: 0.7007312178611755\n",
      "epoch : 7 - step : 3/14 - loss: 0.6833651065826416\n",
      "epoch : 7 - step : 4/14 - loss: 0.7527962327003479\n",
      "epoch : 7 - step : 5/14 - loss: 0.7765830755233765\n",
      "epoch : 7 - step : 6/14 - loss: 0.7452026605606079\n",
      "epoch : 7 - step : 7/14 - loss: 0.6444206237792969\n",
      "epoch : 7 - step : 8/14 - loss: 0.5787153244018555\n",
      "epoch : 7 - step : 9/14 - loss: 0.7111247777938843\n",
      "epoch : 7 - step : 10/14 - loss: 0.8601981997489929\n",
      "epoch : 7 - step : 11/14 - loss: 0.7540444135665894\n",
      "epoch : 7 - step : 12/14 - loss: 0.5802208185195923\n",
      "epoch : 7 - step : 13/14 - loss: 0.7075532078742981\n",
      "Epoch: 7, train_Loss:  0.7100277543067932\n",
      "Epoch: 7, Valid_Loss:  0.7526145726442337\n",
      "epoch : 8 - step : 0/14 - loss: 0.7075003981590271\n",
      "epoch : 8 - step : 1/14 - loss: 0.709829568862915\n",
      "epoch : 8 - step : 2/14 - loss: 0.8038845062255859\n",
      "epoch : 8 - step : 3/14 - loss: 0.6826118230819702\n",
      "epoch : 8 - step : 4/14 - loss: 0.689933717250824\n",
      "epoch : 8 - step : 5/14 - loss: 0.6304298043251038\n",
      "epoch : 8 - step : 6/14 - loss: 0.7745981812477112\n",
      "epoch : 8 - step : 7/14 - loss: 0.6829710006713867\n",
      "epoch : 8 - step : 8/14 - loss: 0.5959429740905762\n",
      "epoch : 8 - step : 9/14 - loss: 0.6833394765853882\n",
      "epoch : 8 - step : 10/14 - loss: 0.7164018154144287\n",
      "epoch : 8 - step : 11/14 - loss: 0.7170193791389465\n",
      "epoch : 8 - step : 12/14 - loss: 0.6809656023979187\n",
      "epoch : 8 - step : 13/14 - loss: 0.7410792708396912\n",
      "Epoch: 8, train_Loss:  0.7011791084493909\n",
      "Epoch: 8, Valid_Loss:  0.758024737238884\n"
     ]
    }
   ],
   "source": [
    "loss_dic = {\"epoch\":[],\"Train\":[], \"Val\":[]}\n",
    "best_loss = 100\n",
    "early_stop_count = 0\n",
    "for epoch in range(1, 99):\n",
    "    \n",
    "    loss_dic['epoch'].append(epoch)\n",
    "    train(epoch, train_loader)\n",
    "    validate(epoch, valid_loader)\n",
    "    scheduler.step()\n",
    "    \n",
    "    if loss_dic['Val'][epoch - 1] > best_loss:\n",
    "        early_stop_count += 1       \n",
    "        if early_stop_count >= 2:\n",
    "            loss_dic_df = pd.DataFrame(loss_dic)\n",
    "            loss_dic_df.to_excel('../../★train/hyeogi/loss.xlsx', index=False)\n",
    "            torch.save(model.state_dict(), f'../../★train/hyeogi/hyeogi_Solar-10.7B-dpo-v1_checkpoint_{epoch}.pth')\n",
    "            break\n",
    "    else:\n",
    "        best_loss = loss_dic['Val'][epoch - 1]\n",
    "        early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcbfb2c-2f94-4de8-a3bb-a7bfc2f514a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
