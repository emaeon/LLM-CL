{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d9d8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3088ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb -Uq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "927c148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wandb\n",
    "\n",
    "# wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d22d3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(project='cl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e9267e0-1905-4e0a-90b0-63e7813128b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/_distutils_hack/__init__.py:55: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig,HfArgumentParser, get_scheduler, set_seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53ef4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'mode_ID':\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "          'seed': 1 ,\n",
    "          'max_seq_len' : 4096,\n",
    "          'epochs': 3,\n",
    "          'lr': 2e-4,\n",
    "          'batch': 4,\n",
    "          'lora_r':8,\n",
    "          'lora_alpha':32,\n",
    "          'target_module':[\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\",\"gate_proj\", \"v_proj\"],\n",
    "          'lora_dropout':0.05,\n",
    "          'lora_tasktype' :'CAUSAL_LM',\n",
    "          'lora_bias' : 'none',\n",
    "          'optimizer': 'paged_adamw_8bit',\n",
    "          'scheduler':'cosine'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516cc735",
   "metadata": {},
   "source": [
    "## Model 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a76dada",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,  # PEFT 설정을 가져오기 위한 함수\n",
    "    get_peft_model,  # PEFT 모델을 가져오기 위한 함수\n",
    "    get_peft_model_state_dict,  # PEFT 모델 상태 사전을 가져오기 위한 함수\n",
    "    set_peft_model_state_dict,  # PEFT 모델 상태 사전을 설정하기 위한 함수\n",
    "    LoraConfig,  # LoRA 모델 구성을 정의하는 클래스\n",
    "    PeftType,  # PEFT 모델의 타입을 정의\n",
    "    PrefixTuningConfig,  # PrefixTuning 모델 구성을 정의하는 클래스\n",
    "    PromptEncoderConfig,  # PromptEncoder 모델 구성을 정의하는 클래스\n",
    "    PeftModel,  # PEFT 모델을 정의하는 클래스\n",
    "    PeftConfig,  # PEFT 모델의 구성을 정의하는 클래스\n",
    ")\n",
    "\n",
    "# PEFT 모델의 타입 설정 (LoRA로 설정)\n",
    "peft_type = PeftType.LORA\n",
    "\n",
    "# LoRA 모델을 위한 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=config['lora_r'],  # LoRA 모델의 r 값\n",
    "    lora_alpha=config['lora_alpha'],  # LoRA 모델의 alpha 값\n",
    "    target_modules=config['target_module'],  # LoRA 모델의 타겟 모듈 리스트\n",
    "    lora_dropout=config['lora_dropout'],  # LoRA 모델의 드롭아웃 비율\n",
    "    bias=config['lora_bias'],  # LoRA 모델의 편향 설정\n",
    "    task_type=config['lora_tasktype']  # LoRA 모델의 태스크 유형\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed05f2fd-0929-4c62-8b7e-2c26eb3b0b69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.38s/it]\n"
     ]
    }
   ],
   "source": [
    "# AutoTokenizer를 사용하여 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['mode_ID'], trust_remote_code=True, eos_token='</s>')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig['mode_ID'],\n",
    "\tdevice_map=\"cuda\",\n",
    "\ttorch_dtype=torch.float16,\n",
    "\ttrust_remote_code=True, \n",
    "\tuse_cache=False,\n",
    "    # attn_implementation='flash_attention_2'\n",
    "\t# quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "model.gradient_checkpointing_enable() # 모델에서 그래디언트 체크포인팅 활성화 (메모리 효율 향상)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7ba1119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phi3 크기 : 3821.1M개의 파라미터\n"
     ]
    }
   ],
   "source": [
    "print(f'Phi3 크기 : {model.num_parameters()/1000**2:.1f}M개의 파라미터')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b199022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "trainable params: 4,456,448 || all params: 3,825,536,000 || trainable%: 0.1165\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training # peft 라이브러리에서 k 비트 학습 준비 함수 임포트\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # CUDA 사용 가능 여부 확인\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)# k 비트 학습을 위해 모델 준비 - prepare_model_for_kbit_training 함수 사용\n",
    "model = get_peft_model(model, peft_config) # PEFT 적용 \n",
    "model = model.to(device) # 모델을 학습 장치 (GPU 등)로 이동\n",
    "model.print_trainable_parameters()# 훈련 가능한 파라미터 출력 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23758421-c94e-463a-a25b-048854782cde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_prompt(user_request, answer):\n",
    "    \n",
    "    conversation = [ {'role': 'user', 'content': user_request},\n",
    "                  {'role': 'assistant', 'content': answer}]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bd1bcc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input final_decision\n",
      "133561  Question:\\nDoes aspirin increase bleeding comp...             no\n",
      "123846  Question:\\nAre measures of socioeconomic posit...             no\n",
      "143951  Question:\\nDoes dialysis within 24 hours of tr...             no\n",
      "79644   Question:\\nIs mild renal pelvic dilatation pre...             no\n",
      "108150  Question:\\nDoes acute blood pressure reduction...             no\n",
      "...                                                   ...            ...\n",
      "47899   Question:\\nDoes inhibition of poly ( ADP-ribos...            yes\n",
      "191520  Question:\\nDoes tumor necrosis factor prevent ...            yes\n",
      "30724   Question:\\nDoes status of hepatic DNA methylom...            yes\n",
      "79443   Question:\\nIs the lower pole of the earlobe an...            yes\n",
      "97016   Question:\\nDo values anglo-american and mexica...            yes\n",
      "\n",
      "[800 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# train 만들기\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "with open('./data/pqaa_train_set.json','r') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "# 데이터프레임에 넣을 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 딕셔너리를 순회하며 데이터프레임용 리스트 생성\n",
    "for num, details in train_data.items():\n",
    "    contexts_with_labels = '\\n'.join([f\"({label}) {context}\" for label, context in zip(details['LABELS'], details['CONTEXTS'])])\n",
    "    input = 'Question:\\n' + details['QUESTION'] + '\\nPlease give me the answer in formats: yes or no' + '\\n' + 'Context:\\n' + contexts_with_labels\n",
    "    row = {\n",
    "        'input' : input,\n",
    "        'final_decision': details['final_decision']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=400, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=400, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df_train = pd.concat([no_df, yes_df])\n",
    "\n",
    "print(combined_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "547594c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    input final_decision\n",
      "50831   Question:\\nIs vincristine-induced neuropathy i...             no\n",
      "108941  Question:\\nDoes topical nutlin-3a decrease pho...             no\n",
      "189150  Question:\\nDo racial differences in withdrawal...             no\n",
      "135688  Question:\\nRegional differences in home food a...             no\n",
      "99209   Question:\\nDoes pantoprazole affect performanc...             no\n",
      "...                                                   ...            ...\n",
      "18397   Question:\\nIs cycling performance decrement gr...            yes\n",
      "104660  Question:\\nDoes knockdown of a proliferation-i...            yes\n",
      "129365  Question:\\nDo elevated angiogenin levels in th...            yes\n",
      "193784  Question:\\nDoes fat-specific transgenic expres...            yes\n",
      "130858  Question:\\nIs reduced muscle strength the majo...            yes\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# valid 만들기\n",
    "del_li = combined_df_train['input'].to_list()\n",
    "df = df[~df['input'].isin(del_li)]\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=50, random_state=42)\n",
    "\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=50, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df_valid = pd.concat([no_df, yes_df])\n",
    "\n",
    "print(combined_df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b255989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   input final_decision\n",
      "2774   Question:\\nDoes trimetazidine modify blood lev...             no\n",
      "4083   Question:\\nDo aDAMTS-5 deficient mice develop ...             no\n",
      "10063  Question:\\nIs brucellosis a major cause of feb...             no\n",
      "3060   Question:\\nDoes sertraline alter the beta-adre...             no\n",
      "7219   Question:\\nAre salivary biomarkers suitable fo...             no\n",
      "...                                                  ...            ...\n",
      "1469   Question:\\nDoes varus malalignment negate the ...            yes\n",
      "3216   Question:\\nDo inflammatory protein levels and ...            yes\n",
      "8590   Question:\\nDoes intraaortic balloon pumping im...            yes\n",
      "4770   Question:\\nDo girls ' childhood trajectories o...            yes\n",
      "6134   Question:\\nIs activation of phospholipase A2 a...            yes\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# test 만들기\n",
    "\n",
    "with open('./data/pqaa_dev_set.json','r') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "# 데이터프레임에 넣을 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 딕셔너리를 순회하며 데이터프레임용 리스트 생성\n",
    "for num, details in test_data.items():\n",
    "    contexts_with_labels = '\\n'.join([f\"({label}) {context}\" for label, context in zip(details['LABELS'], details['CONTEXTS'])])\n",
    "    input = 'Question:\\n' + details['QUESTION'] + '\\nPlease give me the answer in formats: yes or no' + '\\n' + 'Context:\\n' + contexts_with_labels\n",
    "    row = {\n",
    "        'input' : input,\n",
    "        'final_decision': details['final_decision']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=50, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=50, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df_test = pd.concat([no_df, yes_df])\n",
    "\n",
    "print(combined_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bbff1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train = combined_df_train['input']\n",
    "y_train = combined_df_train['final_decision']\n",
    "\n",
    "X_valid = combined_df_valid['input']\n",
    "y_valid = combined_df_valid['final_decision']\n",
    "\n",
    "# test 데이터셋\n",
    "X_test = combined_df_test['input']\n",
    "y_test = combined_df_test['final_decision']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2320b155",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(final_decision\n",
       " no     400\n",
       " yes    400\n",
       " Name: count, dtype: int64,\n",
       " final_decision\n",
       " no     50\n",
       " yes    50\n",
       " Name: count, dtype: int64,\n",
       " final_decision\n",
       " no     50\n",
       " yes    50\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df_train['final_decision'].value_counts(), combined_df_valid['final_decision'].value_counts(), combined_df_test['final_decision'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6abfa3b4-c0fd-494e-a825-abaca4e5497f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data_prompt_list = []\n",
    "for x,y in zip(X_train, y_train):\n",
    "    train_data_prompt_list.append(make_prompt(x,y))\n",
    "\n",
    "valid_data_prompt_list = []\n",
    "for x2,y2 in zip(X_valid, y_valid):\n",
    "    valid_data_prompt_list.append(make_prompt(x2,y2))\n",
    "\n",
    "test_data_prompt_list = []\n",
    "for x3,y3 in zip(X_test, y_test):\n",
    "    test_data_prompt_list.append(make_prompt(x3,y3))\n",
    "    test_data_prompt_list = [test_data.split('<|end|>')[0] + '<|end|>' for test_data in test_data_prompt_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "421e6931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nQuestion:\\nDoes trimetazidine modify blood levels and immunosuppressant effects of cyclosporine A in renal allograft recipients?\\nPlease give me the answer in formats: yes or no\\nContext:\\n(OBJECTIVE) In renal allograft recipients, trimetazidine (Vastarel) was proposed to be associated with the classic immunosuppressant treatments because it displays anti-ischaemic effects which may protect against cyclosporine A nephrotoxicity. The objective of this work was to assess the possibility of coadministering cyclosporin A, Sandimmun, and trimetazidine.\\n(METHODS) Twelve renal transplant patients were selected on the basis of the stability of their cyclosporine A blood concentrations for the previous 3 months. They received trimetazidine, 40 mg twice daily orally for 5 days. Other coadministered drugs were kept unchanged during the study. Before and after trimetazidine administration, cyclosporine A blood concentrations, plasma interleukin-2 and soluble interleukin-2 receptor levels were measured.\\n(RESULTS) The data showed that neither cyclosporin A blood pharmacokinetic parameters, Cmax, tmax, AUC, nor the concentrations of interleukin-2 and soluble interleukin-2 receptors were significantly modified.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_prompt_list[0].split('<|end|>')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54193a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nQuestion:\\nDoes trimetazidine modify blood levels and immunosuppressant effects of cyclosporine A in renal allograft recipients?\\nPlease give me the answer in formats: yes or no\\nContext:\\n(OBJECTIVE) In renal allograft recipients, trimetazidine (Vastarel) was proposed to be associated with the classic immunosuppressant treatments because it displays anti-ischaemic effects which may protect against cyclosporine A nephrotoxicity. The objective of this work was to assess the possibility of coadministering cyclosporin A, Sandimmun, and trimetazidine.\\n(METHODS) Twelve renal transplant patients were selected on the basis of the stability of their cyclosporine A blood concentrations for the previous 3 months. They received trimetazidine, 40 mg twice daily orally for 5 days. Other coadministered drugs were kept unchanged during the study. Before and after trimetazidine administration, cyclosporine A blood concentrations, plasma interleukin-2 and soluble interleukin-2 receptor levels were measured.\\n(RESULTS) The data showed that neither cyclosporin A blood pharmacokinetic parameters, Cmax, tmax, AUC, nor the concentrations of interleukin-2 and soluble interleukin-2 receptors were significantly modified.<|end|>'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_prompt_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d05d3a2-d9ee-43f2-9f40-980669abd577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "89f92546-0270-4385-b977-f75f3892c640",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = Dataset(train_data_prompt_list)\n",
    "valid_dataset = Dataset(valid_data_prompt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "554587bf-a69f-423d-abf1-7a3cbe77746d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch, loader):\n",
    "\n",
    "    model.train()\n",
    "    loss_avg = 0\n",
    "    for i, prompt in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            loss = outputs.loss\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        #loss.backward()\n",
    "        #optimizer.step()\n",
    "        scaler.update()\n",
    "        # print(f\"epoch : {epoch} - step : {i}/{len(loader)} - loss: {loss.item()}\")\n",
    "        loss_avg += loss.item()\n",
    "        \n",
    "        del inputs\n",
    "        del outputs\n",
    "        del loss\n",
    "\n",
    "    # wandb.log({\"loss\": loss_avg/len(loader), \"epoch\": epoch})    \n",
    "    print(f'Epoch: {epoch}, train_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Train'].append(loss_avg/len(loader))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd12d81f-d0a5-4694-9971-504645178cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(epoch,loader):  \n",
    "    model.eval()\n",
    "    loss_avg = 0\n",
    "    with torch.no_grad():       \n",
    "        for i, prompt in enumerate(loader):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss\n",
    "            loss_avg += loss.item()\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "            del loss\n",
    "            \n",
    "    print(f'Epoch: {epoch}, Valid_Loss:  {loss_avg/len(loader)}')\n",
    "    loss_dic['Val'].append(loss_avg/len(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35f933dc-9b98-42ed-88b4-1cf78e7ed602",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_loader = DataLoader(valid_dataset, batch_size=2, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "184ee7b8-d2bf-4153-bea4-bd29fc30bd55",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22213/4029353719.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr = 3e-4)\n",
    "# # optimizer = SGD(model.parameters(), lr=3e-4)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = GradScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5ee90f",
   "metadata": {},
   "source": [
    "# Loss 기반 데이터 정렬 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fe4ef72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_based_sorting(dataset):\n",
    "    # Loss 기반 Dataloader 정렬\n",
    "    data_loss_dict = {}\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prompt in tqdm(dataset):\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, padding=True).to(model.device)\n",
    "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "            loss = outputs.loss.item()\n",
    "            # # print(prompt[0])\n",
    "            # # print(loss)\n",
    "            data_loss_dict[prompt] = loss\n",
    "            # break\n",
    "\n",
    "    sorted_train_dict = dict(sorted(data_loss_dict.items(), key=lambda item: item[1]))\n",
    "\n",
    "    sorted_li = list(sorted_train_dict.keys())\n",
    "    # print(f'sorted_li의 길이 : {len(sorted_li)}')\n",
    "\n",
    "    sorted_train_dataset = Dataset(sorted_li)\n",
    "    # print(type(train_dataset[0]))\n",
    "    sorted_train_loader = DataLoader(sorted_train_dataset, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "    return sorted_train_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae3ee9",
   "metadata": {},
   "source": [
    "# lr 수정 해당 파트 (optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27877355",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1abf8fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = get_scheduler(\n",
    "    name='cosine',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=227,\n",
    "    num_training_steps=15000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5b4791b-bdab-4cf0-b465-e66be73c1ade",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]You are not running the flash-attention implementation, expect numerical differences.\n",
      "100%|██████████| 800/800 [04:32<00:00,  2.94it/s]\n",
      "/tmp/ipykernel_22213/3889949571.py:7: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train_Loss:  2.3611921179294586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 1/10 [13:44<2:03:40, 824.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Valid_Loss:  2.3612437799572943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [04:31<00:00,  2.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, train_Loss:  1.5020258857309818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 2/10 [27:28<1:49:51, 823.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Valid_Loss:  1.3316953828930855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [04:31<00:00,  2.95it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import itertools\n",
    "from transformers import pipeline\n",
    "from sklearn.metrics import accuracy_score \n",
    "\n",
    "loss_dic = {\"epoch\":[],\"Train\":[], \"Val\":[]}\n",
    "best_loss = 100\n",
    "early_stop_count = 0\n",
    "\n",
    "for epoch in tqdm(range(1, 11)):\n",
    "\n",
    "    loss_dic['epoch'].append(epoch)\n",
    "    sorted_train_loader = loss_based_sorting(train_dataset)\n",
    "\n",
    "    train(epoch, sorted_train_loader)\n",
    "    validate(epoch, sorted_train_loader)\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    if loss_dic['Val'][epoch - 1] > best_loss:\n",
    "        early_stop_count += 1       \n",
    "        if early_stop_count >= 2:\n",
    "            loss_dic_df = pd.DataFrame(loss_dic)\n",
    "            loss_dic_df.to_csv('./results/240822_our_loss_5e-4.csv', index=False)\n",
    "            torch.save(model.state_dict(), f'./savedmodel/240822_our_bestmodel_5e-4.pth')\n",
    "            break\n",
    "    else:\n",
    "        best_loss = loss_dic['Val'][epoch - 1]\n",
    "        early_stop_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5301ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping 안됐을때 모델, 결과 따로 저장\n",
    "torch.save(model.state_dict(), './savedmodel/240822_our_bestmodel_5e-4.pth')\n",
    "loss_dic_df = pd.DataFrame(loss_dic)\n",
    "loss_dic_df.to_csv('./results/240822_our_loss_5e-4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ca3ea",
   "metadata": {},
   "source": [
    "# 추론 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "239a64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a87be34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/_distutils_hack/__init__.py:55: UserWarning: Reliance on distutils from stdlib is deprecated. Users must rely on setuptools to provide the distutils module. Avoid importing distutils or import setuptools first, and avoid setting SETUPTOOLS_USE_DISTUTILS=stdlib. Register concerns at https://github.com/pypa/setuptools/issues/new?template=distutils-deprecation.yml\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import  AutoTokenizer, PreTrainedTokenizerFast, AdamW, AutoModelForCausalLM, BitsAndBytesConfig,HfArgumentParser, get_scheduler, set_seed\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, Subset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import cuda\n",
    "from torch.optim import AdamW, SGD\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "import bitsandbytes as bnb\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e7fdb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'mode_ID':\"microsoft/Phi-3-mini-4k-instruct\",\n",
    "          'seed': 1 ,\n",
    "          'max_seq_len' : 4096,\n",
    "          'epochs': 3,\n",
    "          'lr': 2e-4,\n",
    "          'batch': 4,\n",
    "          'lora_r':8,\n",
    "          'lora_alpha':32,\n",
    "          'target_module':[\"q_proj\", \"up_proj\", \"o_proj\", \"k_proj\", \"down_proj\",\"gate_proj\", \"v_proj\"],\n",
    "          'lora_dropout':0.05,\n",
    "          'lora_tasktype' :'CAUSAL_LM',\n",
    "          'lora_bias' : 'none',\n",
    "          'optimizer': 'paged_adamw_8bit',\n",
    "          'scheduler':'cosine'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c6b9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import (\n",
    "    get_peft_config,  # PEFT 설정을 가져오기 위한 함수\n",
    "    get_peft_model,  # PEFT 모델을 가져오기 위한 함수\n",
    "    get_peft_model_state_dict,  # PEFT 모델 상태 사전을 가져오기 위한 함수\n",
    "    set_peft_model_state_dict,  # PEFT 모델 상태 사전을 설정하기 위한 함수\n",
    "    LoraConfig,  # LoRA 모델 구성을 정의하는 클래스\n",
    "    PeftType,  # PEFT 모델의 타입을 정의\n",
    "    PrefixTuningConfig,  # PrefixTuning 모델 구성을 정의하는 클래스\n",
    "    PromptEncoderConfig,  # PromptEncoder 모델 구성을 정의하는 클래스\n",
    "    PeftModel,  # PEFT 모델을 정의하는 클래스\n",
    "    PeftConfig,  # PEFT 모델의 구성을 정의하는 클래스\n",
    ")\n",
    "\n",
    "# PEFT 모델의 타입 설정 (LoRA로 설정)\n",
    "peft_type = PeftType.LORA\n",
    "\n",
    "# LoRA 모델을 위한 설정\n",
    "peft_config = LoraConfig(\n",
    "    r=config['lora_r'],  # LoRA 모델의 r 값\n",
    "    lora_alpha=config['lora_alpha'],  # LoRA 모델의 alpha 값\n",
    "    target_modules=config['target_module'],  # LoRA 모델의 타겟 모듈 리스트\n",
    "    lora_dropout=config['lora_dropout'],  # LoRA 모델의 드롭아웃 비율\n",
    "    bias=config['lora_bias'],  # LoRA 모델의 편향 설정\n",
    "    task_type=config['lora_tasktype']  # LoRA 모델의 태스크 유형\n",
    ")\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bed0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.26s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "\tconfig['mode_ID'],\n",
    "\tdevice_map=\"cuda\",\n",
    "\ttorch_dtype=torch.float16,\n",
    "\ttrust_remote_code=True, \n",
    "\tuse_cache=False,\n",
    "    # attn_implementation='flash_attention_2'\n",
    "\t# quantization_config=bnb_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c155e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9857b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, peft_config) # PEFT 적용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de21425b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22177/1048388050.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./savedmodel/240822_our_bestmodel_3e-4.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./savedmodel/240822_our_bestmodel_5e-4.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d9c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoTokenizer를 사용하여 토크나이저 생성\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['mode_ID'], trust_remote_code=True, eos_token='</s>')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "def make_prompt(user_request, answer):\n",
    "    \n",
    "    conversation = [ {'role': 'user', 'content': user_request},\n",
    "                  {'role': 'assistant', 'content': answer}]\n",
    "    prompt = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
    "    return prompt\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('./data/pqaa_dev_set.json','r') as f:\n",
    "    test_data = json.load(f)\n",
    "    \n",
    "# 데이터프레임에 넣을 리스트 초기화\n",
    "rows = []\n",
    "\n",
    "# 딕셔너리를 순회하며 데이터프레임용 리스트 생성\n",
    "for num, details in test_data.items():\n",
    "    contexts_with_labels = '\\n'.join([f\"({label}) {context}\" for label, context in zip(details['LABELS'], details['CONTEXTS'])])\n",
    "    input = 'Question:\\n' + details['QUESTION'] + '\\nPlease give me the answer in formats: yes or no' + '\\n' + 'Context:\\n' + contexts_with_labels\n",
    "    row = {\n",
    "        'input' : input,\n",
    "        'final_decision': details['final_decision']\n",
    "    }\n",
    "    rows.append(row)\n",
    "\n",
    "# 데이터프레임 생성\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "no_df = df[df['final_decision'] == 'no'].sample(n=50, random_state=42)\n",
    "\n",
    "# 'yes'인 값 10000개 추출\n",
    "yes_df = df[df['final_decision'] == 'yes'].sample(n=50, random_state=42)\n",
    "\n",
    "# 두 데이터 프레임 합치기\n",
    "combined_df_test = pd.concat([no_df, yes_df])\n",
    "\n",
    "# print(combined_df_test)\n",
    "\n",
    "X_test = combined_df_test['input']\n",
    "y_test = combined_df_test['final_decision']\n",
    "\n",
    "test_data_prompt_list = []\n",
    "for x3,y3 in zip(X_test, y_test):\n",
    "    test_data_prompt_list.append(make_prompt(x3,y3))\n",
    "    test_data_prompt_list = [test_data.split('<|end|>')[0] + '<|end|>\\n<|assistant|>\\n' for test_data in test_data_prompt_list]\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a001c6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(test_data_prompt_list)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8cef43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "def test(loader):\n",
    "    output_li = []\n",
    "    model.eval()\n",
    "\n",
    "    pipe = pipeline( \n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    ) \n",
    "\n",
    "    generation_args = { \n",
    "        \"max_new_tokens\": 500, \n",
    "        \"return_full_text\": False, \n",
    "        \"temperature\": 0.5, \n",
    "        \"do_sample\": False, \n",
    "    } \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for output in tqdm(pipe(loader, **generation_args)):\n",
    "            output_li.append(output)\n",
    "            \n",
    "    return output_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6943f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'LlamaForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM', 'Phi3ForCausalLM'].\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/work/anaconda3/envs/CL/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "100%|██████████| 100/100 [00:40<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': ' no'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "outputs = test(test_dataset)\n",
    "print(outputs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e3569db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_li = []\n",
    "for output in outputs:\n",
    "    # print(output[0].get('generated_text').strip())\n",
    "    pred_li.append(output[0].get('generated_text').strip().lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86001ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'no',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'no',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes',\n",
       " 'yes']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19b5fc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>true</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2774</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4083</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10063</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3060</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7219</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1469</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3216</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8590</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4770</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6134</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      true pred\n",
       "2774    no   no\n",
       "4083    no  yes\n",
       "10063   no   no\n",
       "3060    no   no\n",
       "7219    no   no\n",
       "...    ...  ...\n",
       "1469   yes   no\n",
       "3216   yes  yes\n",
       "8590   yes  yes\n",
       "4770   yes  yes\n",
       "6134   yes  yes\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({'true': y_test, 'pred':pred_li})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef06f9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(df['true'],df['pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca279235",
   "metadata": {},
   "source": [
    "# Acc 기록\n",
    "\n",
    "- 1e-5 : 0.8\n",
    "- 3e-5 : 0.77\n",
    "- 5e-5 : 0.83\n",
    "- 1e-4 : 0.86\n",
    "- 3e-4 : 0.85\n",
    "- 5e-4 :\n",
    "- 1e-3 :\n",
    "- 3e-3 :\n",
    "- 5e-3 :"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.1 (NGC 23.07/Python 3.10) on Backend.AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
